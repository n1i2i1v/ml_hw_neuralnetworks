{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "final_assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs-j0egFFk8I",
        "colab_type": "text"
      },
      "source": [
        "This requires you to write a 2D GAN game. I let you to get into the topic yourself, whitout any explonations from my side. You can watch internet, read papers and tutorials (fun, fun, fun)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9W36KFrFk8M",
        "colab_type": "text"
      },
      "source": [
        "### Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqrVKiEnFk8P",
        "colab_type": "text"
      },
      "source": [
        "I want you to implement a simple 2D GAN game. The kind of animation, I want to see is like in [this video](https://www.youtube.com/watch?v=KeJINHjyzOU&feature=youtu.be&t=15m38s) at 15:36 or in [here](https://habrahabr.ru/post/275429/) **but in 2D**. You can google, search code at github, whatever, but the network should be based on TensoFlow.\n",
        "\n",
        "Basically you will need to come up with true distribution $P$, say mixture of gaussians (surprise me), sample some data from it. Visualize it as a heatmap. To visualize $G$ density you can fix $N$ noise vectors $\\{z_i\\} \\quad i=1,\\dots, N$ and draw a circle for each $G(z_i)$. It is also funny to visualize discriminator as a vector field (can be done with `plt.arrow`, `plt.quiver` plot). Look how it should be in the middle of [this page](http://www.inference.vc/an-alternative-update-rule-for-generative-adversarial-networks/).\n",
        "\n",
        "Please, make sure your code works if 'Run All' is pressed and it draws some animation.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nz80BkdFk8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU2eorOkFk8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "8fb40437-4882-4fab-9483-72c2bc69217d"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist=input_data.read_data_sets(\"MNIST_data\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 09:51:39.326717 140090319914880 deprecation.py:323] From <ipython-input-3-946b340fa2a5>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0807 09:51:39.328257 140090319914880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0807 09:51:39.329791 140090319914880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0807 09:51:39.731797 140090319914880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0807 09:51:40.109057 140090319914880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0807 09:51:40.424135 140090319914880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAf6chFMFk8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(inpt, reuse=None):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "        l1 = tf.layers.dense(inputs = inpt, units=128, activation = tf.nn.leaky_relu)\n",
        "        l2 = tf.layers.dense(inputs = l1, units=256, activation = tf.nn.leaky_relu)\n",
        "        l0 = tf.layers.dense(inputs = l2, units=784, activation = tf.nn.tanh)\n",
        "            \n",
        "    return l0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGfJYoJJFk8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(inpt, reuse=None):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "        l1 = tf.layers.dense(inputs = inpt, units=128, activation = tf.nn.leaky_relu)\n",
        "        l2 = tf.layers.dense(inputs = l1, units=128, activation = tf.nn.leaky_relu)\n",
        "        logits = tf.layers.dense(inputs = l2, units=1)\n",
        "        l0 = tf.sigmoid(logits)\n",
        "            \n",
        "    return l0, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM5iZkr-Fk8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh8w5FyRFk8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tX = tf.placeholder(tf.float32,shape=[None,784])\n",
        "tZ = tf.placeholder(tf.float32,shape=[None,100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCCF9_eBFk8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f7d4c5d5-68eb-47da-e4f7-165d669a4aed"
      },
      "source": [
        "gen = generator(tZ)\n",
        "d_real, d_logits_real = discriminator(tX)\n",
        "d_fake, d_logits_fake = discriminator(gen, reuse = True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 09:51:46.342123 140090319914880 deprecation.py:323] From <ipython-input-4-724feb13abe4>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0807 09:51:46.357612 140090319914880 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw735383Fk83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cost_function(logits,labels):\n",
        "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4PTc9bpFk88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "ce4ae95d-77e7-41e6-c749-291b6e8f428a"
      },
      "source": [
        "d_real_loss = cost_function(d_logits_real, tf.ones_like(d_logits_real)*0.95)\n",
        "d_fake_loss= cost_function(d_logits_fake, tf.zeros_like(d_logits_real))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 09:51:49.986807 140090319914880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLcRetTSFk9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_loss= cost_function(d_logits_fake, tf.ones_like(d_logits_fake))\n",
        "disc_loss=d_real_loss+d_fake_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNPnVeUkFk9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainables=tf.trainable_variables()\n",
        "gen_trainers=[var for var in trainables if 'generator' in var.name]\n",
        "disc_trainers=[var for var in trainables if 'discriminator' in var.name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbzWojl0Fk9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 1000\n",
        "batch_size = 100\n",
        "pretrained = 400"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbGXokd-Fk9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_op = tf.train.AdamOptimizer(learning_rate).minimize(gen_loss, var_list=gen_trainers)\n",
        "disc_op = tf.train.AdamOptimizer(learning_rate).minimize(disc_loss, var_list=disc_trainers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvgDeb-Fk9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rVm540AFk9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('imaginator/'):\n",
        "    os.makedirs('imaginator/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kGJb8dFk9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXUFePfrFk9b",
        "colab_type": "code",
        "outputId": "c397938c-eeeb-4ea5-ef97-e47f83979897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generated = []\n",
        "\n",
        "import time\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    disc_cost = []\n",
        "    gen_cost = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        num_batches = mnist.train.num_examples//batch_size\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for i in range(num_batches):\n",
        "            batch = mnist.train.next_batch(batch_size)\n",
        "            batch_X = batch[0].reshape((batch_size, 784))\n",
        "            batch_X = batch_X*2-1\n",
        "            batch_Z = np.random.uniform(-1,1,size=(batch_size, 100))\n",
        "            \n",
        "            #train\n",
        "            _, d_loss = sess.run([disc_op, disc_loss], feed_dict={tX:batch_X, tZ:batch_Z})\n",
        "            _, g_loss = sess.run([gen_op, gen_loss], feed_dict={tZ:batch_Z})\n",
        "            \n",
        "            disc_cost.append(d_loss)\n",
        "            gen_cost.append(g_loss) \n",
        "            \n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, epochs, time.time() - start_time))\n",
        "        print(\"The Discriminator loss is {} and the Generator loss is {}.\".format(d_loss, g_loss, time.time() - start_time))\n",
        "\n",
        "        \n",
        "        if epoch%100 == 0 and epoch>= pretrained:\n",
        "            print(\"Now Images are being generated\")\n",
        "            sample=np.random.uniform(-1,1,size=(1,100))\n",
        "            gen_sample=sess.run(generator(tZ,reuse=True), feed_dict={tZ:sample})\n",
        "            img = plt.imshow(gen_sample.reshape(28,28))\n",
        "            generated.append(img) #was running this in google collab so did this also:DD\n",
        "            plt.savefig(f\"imaginator/Sample{epoch}_{np.abs(np.around(float(np.random.randn(1)),2))}.png\") "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 1000 took 7.287s\n",
            "The Discriminator loss is 1.297804832458496 and the Generator loss is 14.245400428771973.\n",
            "Epoch 2 of 1000 took 6.989s\n",
            "The Discriminator loss is 0.3471933603286743 and the Generator loss is 12.045433044433594.\n",
            "Epoch 3 of 1000 took 7.147s\n",
            "The Discriminator loss is 0.40926411747932434 and the Generator loss is 4.607583045959473.\n",
            "Epoch 4 of 1000 took 6.937s\n",
            "The Discriminator loss is 0.6104105710983276 and the Generator loss is 4.330989837646484.\n",
            "Epoch 5 of 1000 took 6.873s\n",
            "The Discriminator loss is 0.7037864327430725 and the Generator loss is 4.225381374359131.\n",
            "Epoch 6 of 1000 took 6.756s\n",
            "The Discriminator loss is 0.3617555499076843 and the Generator loss is 5.291323661804199.\n",
            "Epoch 7 of 1000 took 6.776s\n",
            "The Discriminator loss is 0.9216457605361938 and the Generator loss is 4.633940696716309.\n",
            "Epoch 8 of 1000 took 6.896s\n",
            "The Discriminator loss is 0.7716177701950073 and the Generator loss is 11.898930549621582.\n",
            "Epoch 9 of 1000 took 7.419s\n",
            "The Discriminator loss is 0.6956597566604614 and the Generator loss is 4.6286091804504395.\n",
            "Epoch 10 of 1000 took 7.938s\n",
            "The Discriminator loss is 0.5910298824310303 and the Generator loss is 4.399423599243164.\n",
            "Epoch 11 of 1000 took 8.024s\n",
            "The Discriminator loss is 0.4476751685142517 and the Generator loss is 3.3652307987213135.\n",
            "Epoch 12 of 1000 took 7.933s\n",
            "The Discriminator loss is 0.34719181060791016 and the Generator loss is 7.081735610961914.\n",
            "Epoch 13 of 1000 took 7.839s\n",
            "The Discriminator loss is 1.1619350910186768 and the Generator loss is 4.862029075622559.\n",
            "Epoch 14 of 1000 took 7.850s\n",
            "The Discriminator loss is 0.4908291697502136 and the Generator loss is 5.2204084396362305.\n",
            "Epoch 15 of 1000 took 7.805s\n",
            "The Discriminator loss is 0.8018947839736938 and the Generator loss is 3.7520792484283447.\n",
            "Epoch 16 of 1000 took 7.520s\n",
            "The Discriminator loss is 1.2195621728897095 and the Generator loss is 4.074504852294922.\n",
            "Epoch 17 of 1000 took 7.560s\n",
            "The Discriminator loss is 0.8438382744789124 and the Generator loss is 4.520908355712891.\n",
            "Epoch 18 of 1000 took 7.605s\n",
            "The Discriminator loss is 0.5469538569450378 and the Generator loss is 5.605972766876221.\n",
            "Epoch 19 of 1000 took 7.754s\n",
            "The Discriminator loss is 0.6506193280220032 and the Generator loss is 3.6618010997772217.\n",
            "Epoch 20 of 1000 took 7.788s\n",
            "The Discriminator loss is 0.5908183455467224 and the Generator loss is 4.292114734649658.\n",
            "Epoch 21 of 1000 took 7.596s\n",
            "The Discriminator loss is 1.5230847597122192 and the Generator loss is 4.972595691680908.\n",
            "Epoch 22 of 1000 took 7.441s\n",
            "The Discriminator loss is 0.5862739086151123 and the Generator loss is 4.112623691558838.\n",
            "Epoch 23 of 1000 took 7.963s\n",
            "The Discriminator loss is 0.5421386957168579 and the Generator loss is 3.7586047649383545.\n",
            "Epoch 24 of 1000 took 8.075s\n",
            "The Discriminator loss is 0.528606653213501 and the Generator loss is 4.24210262298584.\n",
            "Epoch 25 of 1000 took 7.756s\n",
            "The Discriminator loss is 0.5964263081550598 and the Generator loss is 4.100636959075928.\n",
            "Epoch 26 of 1000 took 7.841s\n",
            "The Discriminator loss is 0.8710206747055054 and the Generator loss is 5.756635665893555.\n",
            "Epoch 27 of 1000 took 7.754s\n",
            "The Discriminator loss is 0.8108576536178589 and the Generator loss is 3.881622314453125.\n",
            "Epoch 28 of 1000 took 7.695s\n",
            "The Discriminator loss is 0.8211706876754761 and the Generator loss is 5.044145584106445.\n",
            "Epoch 29 of 1000 took 7.782s\n",
            "The Discriminator loss is 1.0933887958526611 and the Generator loss is 5.20417594909668.\n",
            "Epoch 30 of 1000 took 7.499s\n",
            "The Discriminator loss is 0.6749728322029114 and the Generator loss is 3.2057037353515625.\n",
            "Epoch 31 of 1000 took 7.030s\n",
            "The Discriminator loss is 0.5320940017700195 and the Generator loss is 4.992722511291504.\n",
            "Epoch 32 of 1000 took 7.132s\n",
            "The Discriminator loss is 1.434757947921753 and the Generator loss is 4.929344654083252.\n",
            "Epoch 33 of 1000 took 7.214s\n",
            "The Discriminator loss is 0.9867877960205078 and the Generator loss is 4.456010341644287.\n",
            "Epoch 34 of 1000 took 7.262s\n",
            "The Discriminator loss is 0.512508749961853 and the Generator loss is 3.446744441986084.\n",
            "Epoch 35 of 1000 took 7.134s\n",
            "The Discriminator loss is 1.0403730869293213 and the Generator loss is 3.3913068771362305.\n",
            "Epoch 36 of 1000 took 7.124s\n",
            "The Discriminator loss is 0.5416436791419983 and the Generator loss is 3.162472724914551.\n",
            "Epoch 37 of 1000 took 7.133s\n",
            "The Discriminator loss is 1.0069400072097778 and the Generator loss is 3.2569327354431152.\n",
            "Epoch 38 of 1000 took 6.954s\n",
            "The Discriminator loss is 0.5557364821434021 and the Generator loss is 3.470036506652832.\n",
            "Epoch 39 of 1000 took 7.016s\n",
            "The Discriminator loss is 0.9244781732559204 and the Generator loss is 3.3619768619537354.\n",
            "Epoch 40 of 1000 took 6.877s\n",
            "The Discriminator loss is 0.6937817335128784 and the Generator loss is 4.541261196136475.\n",
            "Epoch 41 of 1000 took 7.041s\n",
            "The Discriminator loss is 1.1204530000686646 and the Generator loss is 3.233403205871582.\n",
            "Epoch 42 of 1000 took 6.883s\n",
            "The Discriminator loss is 0.6373612284660339 and the Generator loss is 3.642977714538574.\n",
            "Epoch 43 of 1000 took 7.061s\n",
            "The Discriminator loss is 0.8201383352279663 and the Generator loss is 3.210397720336914.\n",
            "Epoch 44 of 1000 took 7.071s\n",
            "The Discriminator loss is 0.4751597046852112 and the Generator loss is 5.82026481628418.\n",
            "Epoch 45 of 1000 took 7.072s\n",
            "The Discriminator loss is 0.6414939761161804 and the Generator loss is 4.772280693054199.\n",
            "Epoch 46 of 1000 took 6.769s\n",
            "The Discriminator loss is 0.8302677273750305 and the Generator loss is 3.6008987426757812.\n",
            "Epoch 47 of 1000 took 6.819s\n",
            "The Discriminator loss is 1.1896233558654785 and the Generator loss is 3.1804299354553223.\n",
            "Epoch 48 of 1000 took 6.824s\n",
            "The Discriminator loss is 0.6409651041030884 and the Generator loss is 3.411433219909668.\n",
            "Epoch 49 of 1000 took 7.275s\n",
            "The Discriminator loss is 1.3305354118347168 and the Generator loss is 3.1654000282287598.\n",
            "Epoch 50 of 1000 took 7.364s\n",
            "The Discriminator loss is 1.2963796854019165 and the Generator loss is 2.5412774085998535.\n",
            "Epoch 51 of 1000 took 7.407s\n",
            "The Discriminator loss is 0.941076934337616 and the Generator loss is 4.066295623779297.\n",
            "Epoch 52 of 1000 took 7.338s\n",
            "The Discriminator loss is 1.051771640777588 and the Generator loss is 2.7600066661834717.\n",
            "Epoch 53 of 1000 took 7.321s\n",
            "The Discriminator loss is 0.7022484540939331 and the Generator loss is 2.641477584838867.\n",
            "Epoch 54 of 1000 took 7.475s\n",
            "The Discriminator loss is 0.8388153314590454 and the Generator loss is 5.5485405921936035.\n",
            "Epoch 55 of 1000 took 7.519s\n",
            "The Discriminator loss is 1.2459551095962524 and the Generator loss is 5.048275470733643.\n",
            "Epoch 56 of 1000 took 7.426s\n",
            "The Discriminator loss is 1.170413851737976 and the Generator loss is 2.8697915077209473.\n",
            "Epoch 57 of 1000 took 7.317s\n",
            "The Discriminator loss is 0.8012891411781311 and the Generator loss is 5.2265167236328125.\n",
            "Epoch 58 of 1000 took 7.460s\n",
            "The Discriminator loss is 0.6446346044540405 and the Generator loss is 6.632143497467041.\n",
            "Epoch 59 of 1000 took 7.481s\n",
            "The Discriminator loss is 0.8506090641021729 and the Generator loss is 2.90250301361084.\n",
            "Epoch 60 of 1000 took 7.368s\n",
            "The Discriminator loss is 1.2493482828140259 and the Generator loss is 3.1144931316375732.\n",
            "Epoch 61 of 1000 took 7.448s\n",
            "The Discriminator loss is 1.5288443565368652 and the Generator loss is 4.06282377243042.\n",
            "Epoch 62 of 1000 took 7.385s\n",
            "The Discriminator loss is 0.7581040263175964 and the Generator loss is 4.582663059234619.\n",
            "Epoch 63 of 1000 took 7.442s\n",
            "The Discriminator loss is 0.6321856379508972 and the Generator loss is 4.044212341308594.\n",
            "Epoch 64 of 1000 took 7.425s\n",
            "The Discriminator loss is 0.7428528666496277 and the Generator loss is 3.805643081665039.\n",
            "Epoch 65 of 1000 took 7.657s\n",
            "The Discriminator loss is 0.7378435134887695 and the Generator loss is 2.9889137744903564.\n",
            "Epoch 66 of 1000 took 7.999s\n",
            "The Discriminator loss is 1.1457831859588623 and the Generator loss is 2.6622474193573.\n",
            "Epoch 67 of 1000 took 7.483s\n",
            "The Discriminator loss is 0.8914851546287537 and the Generator loss is 3.8654544353485107.\n",
            "Epoch 68 of 1000 took 7.395s\n",
            "The Discriminator loss is 1.3378469944000244 and the Generator loss is 2.0675642490386963.\n",
            "Epoch 69 of 1000 took 7.333s\n",
            "The Discriminator loss is 0.9065265655517578 and the Generator loss is 2.3067994117736816.\n",
            "Epoch 70 of 1000 took 7.320s\n",
            "The Discriminator loss is 1.0330533981323242 and the Generator loss is 5.557534217834473.\n",
            "Epoch 71 of 1000 took 7.350s\n",
            "The Discriminator loss is 0.8527809977531433 and the Generator loss is 2.701660394668579.\n",
            "Epoch 72 of 1000 took 7.728s\n",
            "The Discriminator loss is 1.7337008714675903 and the Generator loss is 5.199132919311523.\n",
            "Epoch 73 of 1000 took 7.323s\n",
            "The Discriminator loss is 1.099838137626648 and the Generator loss is 2.1760644912719727.\n",
            "Epoch 74 of 1000 took 7.279s\n",
            "The Discriminator loss is 0.5967494249343872 and the Generator loss is 7.356773853302002.\n",
            "Epoch 75 of 1000 took 7.320s\n",
            "The Discriminator loss is 1.103869915008545 and the Generator loss is 2.0323410034179688.\n",
            "Epoch 76 of 1000 took 7.223s\n",
            "The Discriminator loss is 1.0300475358963013 and the Generator loss is 3.4553475379943848.\n",
            "Epoch 77 of 1000 took 7.109s\n",
            "The Discriminator loss is 1.105064868927002 and the Generator loss is 6.0282721519470215.\n",
            "Epoch 78 of 1000 took 7.121s\n",
            "The Discriminator loss is 1.1261006593704224 and the Generator loss is 3.0634031295776367.\n",
            "Epoch 79 of 1000 took 7.089s\n",
            "The Discriminator loss is 1.3579353094100952 and the Generator loss is 1.8153257369995117.\n",
            "Epoch 80 of 1000 took 7.158s\n",
            "The Discriminator loss is 0.8605774641036987 and the Generator loss is 2.4846303462982178.\n",
            "Epoch 81 of 1000 took 7.164s\n",
            "The Discriminator loss is 0.8551595211029053 and the Generator loss is 3.146350622177124.\n",
            "Epoch 82 of 1000 took 6.997s\n",
            "The Discriminator loss is 0.7977342009544373 and the Generator loss is 6.416241645812988.\n",
            "Epoch 83 of 1000 took 6.992s\n",
            "The Discriminator loss is 1.1579653024673462 and the Generator loss is 1.934066891670227.\n",
            "Epoch 84 of 1000 took 7.026s\n",
            "The Discriminator loss is 0.9974005222320557 and the Generator loss is 4.23280143737793.\n",
            "Epoch 85 of 1000 took 6.997s\n",
            "The Discriminator loss is 1.0330510139465332 and the Generator loss is 3.4714195728302.\n",
            "Epoch 86 of 1000 took 6.903s\n",
            "The Discriminator loss is 1.0056779384613037 and the Generator loss is 5.4052910804748535.\n",
            "Epoch 87 of 1000 took 6.889s\n",
            "The Discriminator loss is 1.6012543439865112 and the Generator loss is 1.769008755683899.\n",
            "Epoch 88 of 1000 took 7.010s\n",
            "The Discriminator loss is 0.8000030517578125 and the Generator loss is 7.672818660736084.\n",
            "Epoch 89 of 1000 took 7.056s\n",
            "The Discriminator loss is 1.8201525211334229 and the Generator loss is 1.475600242614746.\n",
            "Epoch 90 of 1000 took 6.996s\n",
            "The Discriminator loss is 1.0751575231552124 and the Generator loss is 2.0952484607696533.\n",
            "Epoch 91 of 1000 took 7.203s\n",
            "The Discriminator loss is 1.5058588981628418 and the Generator loss is 1.5054396390914917.\n",
            "Epoch 92 of 1000 took 7.069s\n",
            "The Discriminator loss is 1.3472422361373901 and the Generator loss is 2.5565309524536133.\n",
            "Epoch 93 of 1000 took 7.144s\n",
            "The Discriminator loss is 1.0566587448120117 and the Generator loss is 1.7059037685394287.\n",
            "Epoch 94 of 1000 took 7.069s\n",
            "The Discriminator loss is 0.9361159801483154 and the Generator loss is 4.139588832855225.\n",
            "Epoch 95 of 1000 took 7.063s\n",
            "The Discriminator loss is 1.098266363143921 and the Generator loss is 2.626894235610962.\n",
            "Epoch 96 of 1000 took 7.085s\n",
            "The Discriminator loss is 0.9407234191894531 and the Generator loss is 1.893897294998169.\n",
            "Epoch 97 of 1000 took 7.216s\n",
            "The Discriminator loss is 0.998649001121521 and the Generator loss is 2.366751194000244.\n",
            "Epoch 98 of 1000 took 7.141s\n",
            "The Discriminator loss is 1.3741331100463867 and the Generator loss is 3.4095067977905273.\n",
            "Epoch 99 of 1000 took 7.069s\n",
            "The Discriminator loss is 1.390348196029663 and the Generator loss is 2.791367769241333.\n",
            "Epoch 100 of 1000 took 7.002s\n",
            "The Discriminator loss is 1.106522798538208 and the Generator loss is 3.785382032394409.\n",
            "Epoch 101 of 1000 took 6.895s\n",
            "The Discriminator loss is 1.2793751955032349 and the Generator loss is 1.2758525609970093.\n",
            "Epoch 102 of 1000 took 6.872s\n",
            "The Discriminator loss is 1.0126566886901855 and the Generator loss is 7.551680088043213.\n",
            "Epoch 103 of 1000 took 6.929s\n",
            "The Discriminator loss is 1.4893394708633423 and the Generator loss is 2.335452079772949.\n",
            "Epoch 104 of 1000 took 7.129s\n",
            "The Discriminator loss is 1.036984920501709 and the Generator loss is 3.2493350505828857.\n",
            "Epoch 105 of 1000 took 7.286s\n",
            "The Discriminator loss is 0.9436002969741821 and the Generator loss is 2.534803867340088.\n",
            "Epoch 106 of 1000 took 7.123s\n",
            "The Discriminator loss is 0.9956409931182861 and the Generator loss is 3.506415367126465.\n",
            "Epoch 107 of 1000 took 6.968s\n",
            "The Discriminator loss is 1.3969546556472778 and the Generator loss is 4.574375629425049.\n",
            "Epoch 108 of 1000 took 7.243s\n",
            "The Discriminator loss is 1.6990039348602295 and the Generator loss is 1.6161712408065796.\n",
            "Epoch 109 of 1000 took 7.590s\n",
            "The Discriminator loss is 0.7869176268577576 and the Generator loss is 3.842127799987793.\n",
            "Epoch 110 of 1000 took 7.598s\n",
            "The Discriminator loss is 1.2144763469696045 and the Generator loss is 1.8805118799209595.\n",
            "Epoch 111 of 1000 took 7.402s\n",
            "The Discriminator loss is 0.9265512228012085 and the Generator loss is 1.7987943887710571.\n",
            "Epoch 112 of 1000 took 7.302s\n",
            "The Discriminator loss is 0.7172329425811768 and the Generator loss is 4.23325777053833.\n",
            "Epoch 113 of 1000 took 7.341s\n",
            "The Discriminator loss is 1.076075792312622 and the Generator loss is 1.6280837059020996.\n",
            "Epoch 114 of 1000 took 7.342s\n",
            "The Discriminator loss is 1.4064266681671143 and the Generator loss is 1.2154256105422974.\n",
            "Epoch 115 of 1000 took 7.464s\n",
            "The Discriminator loss is 0.5532482862472534 and the Generator loss is 5.492453575134277.\n",
            "Epoch 116 of 1000 took 7.121s\n",
            "The Discriminator loss is 0.8158578872680664 and the Generator loss is 9.220852851867676.\n",
            "Epoch 117 of 1000 took 7.195s\n",
            "The Discriminator loss is 0.8966422080993652 and the Generator loss is 4.2118635177612305.\n",
            "Epoch 118 of 1000 took 7.157s\n",
            "The Discriminator loss is 1.1391792297363281 and the Generator loss is 2.1128597259521484.\n",
            "Epoch 119 of 1000 took 7.071s\n",
            "The Discriminator loss is 1.2115561962127686 and the Generator loss is 3.9264516830444336.\n",
            "Epoch 120 of 1000 took 7.138s\n",
            "The Discriminator loss is 1.4419809579849243 and the Generator loss is 3.9302892684936523.\n",
            "Epoch 121 of 1000 took 7.161s\n",
            "The Discriminator loss is 1.15486478805542 and the Generator loss is 4.050413131713867.\n",
            "Epoch 122 of 1000 took 7.062s\n",
            "The Discriminator loss is 1.4309515953063965 and the Generator loss is 4.820901393890381.\n",
            "Epoch 123 of 1000 took 7.544s\n",
            "The Discriminator loss is 0.9279356002807617 and the Generator loss is 2.8165371417999268.\n",
            "Epoch 124 of 1000 took 7.322s\n",
            "The Discriminator loss is 1.4644405841827393 and the Generator loss is 1.8845866918563843.\n",
            "Epoch 125 of 1000 took 7.360s\n",
            "The Discriminator loss is 0.9946588277816772 and the Generator loss is 2.889103651046753.\n",
            "Epoch 126 of 1000 took 6.911s\n",
            "The Discriminator loss is 0.9700639247894287 and the Generator loss is 2.106917381286621.\n",
            "Epoch 127 of 1000 took 7.036s\n",
            "The Discriminator loss is 1.4278008937835693 and the Generator loss is 1.815101981163025.\n",
            "Epoch 128 of 1000 took 6.911s\n",
            "The Discriminator loss is 1.4564406871795654 and the Generator loss is 2.5691888332366943.\n",
            "Epoch 129 of 1000 took 6.970s\n",
            "The Discriminator loss is 1.410306453704834 and the Generator loss is 2.548916816711426.\n",
            "Epoch 130 of 1000 took 7.048s\n",
            "The Discriminator loss is 1.1923463344573975 and the Generator loss is 2.6942968368530273.\n",
            "Epoch 131 of 1000 took 6.975s\n",
            "The Discriminator loss is 1.0852437019348145 and the Generator loss is 3.862675428390503.\n",
            "Epoch 132 of 1000 took 6.989s\n",
            "The Discriminator loss is 1.6215717792510986 and the Generator loss is 2.622798442840576.\n",
            "Epoch 133 of 1000 took 7.097s\n",
            "The Discriminator loss is 0.9020742177963257 and the Generator loss is 2.2633657455444336.\n",
            "Epoch 134 of 1000 took 7.037s\n",
            "The Discriminator loss is 1.0628547668457031 and the Generator loss is 1.762892723083496.\n",
            "Epoch 135 of 1000 took 7.016s\n",
            "The Discriminator loss is 1.2295490503311157 and the Generator loss is 2.3072543144226074.\n",
            "Epoch 136 of 1000 took 6.852s\n",
            "The Discriminator loss is 1.3994193077087402 and the Generator loss is 1.8762437105178833.\n",
            "Epoch 137 of 1000 took 6.796s\n",
            "The Discriminator loss is 0.6452323198318481 and the Generator loss is 6.109082221984863.\n",
            "Epoch 138 of 1000 took 7.005s\n",
            "The Discriminator loss is 1.6634465456008911 and the Generator loss is 4.049129486083984.\n",
            "Epoch 139 of 1000 took 7.052s\n",
            "The Discriminator loss is 2.2834455966949463 and the Generator loss is 1.9616076946258545.\n",
            "Epoch 140 of 1000 took 6.858s\n",
            "The Discriminator loss is 1.0481014251708984 and the Generator loss is 4.709907531738281.\n",
            "Epoch 141 of 1000 took 6.889s\n",
            "The Discriminator loss is 1.5732862949371338 and the Generator loss is 1.7241483926773071.\n",
            "Epoch 142 of 1000 took 6.915s\n",
            "The Discriminator loss is 2.332821846008301 and the Generator loss is 3.7387542724609375.\n",
            "Epoch 143 of 1000 took 6.765s\n",
            "The Discriminator loss is 1.5231512784957886 and the Generator loss is 2.0443015098571777.\n",
            "Epoch 144 of 1000 took 6.866s\n",
            "The Discriminator loss is 1.5651781558990479 and the Generator loss is 4.177005767822266.\n",
            "Epoch 145 of 1000 took 6.928s\n",
            "The Discriminator loss is 1.783785104751587 and the Generator loss is 1.488351583480835.\n",
            "Epoch 146 of 1000 took 7.007s\n",
            "The Discriminator loss is 1.1474840641021729 and the Generator loss is 3.2914676666259766.\n",
            "Epoch 147 of 1000 took 6.926s\n",
            "The Discriminator loss is 0.9383866786956787 and the Generator loss is 4.327511310577393.\n",
            "Epoch 148 of 1000 took 6.990s\n",
            "The Discriminator loss is 1.1941014528274536 and the Generator loss is 2.2902400493621826.\n",
            "Epoch 149 of 1000 took 7.040s\n",
            "The Discriminator loss is 1.2122528553009033 and the Generator loss is 2.804621696472168.\n",
            "Epoch 150 of 1000 took 6.999s\n",
            "The Discriminator loss is 0.8676007986068726 and the Generator loss is 2.8353981971740723.\n",
            "Epoch 151 of 1000 took 7.203s\n",
            "The Discriminator loss is 0.8074057102203369 and the Generator loss is 3.5371334552764893.\n",
            "Epoch 152 of 1000 took 7.245s\n",
            "The Discriminator loss is 1.1989381313323975 and the Generator loss is 3.477025032043457.\n",
            "Epoch 153 of 1000 took 7.524s\n",
            "The Discriminator loss is 0.9145663976669312 and the Generator loss is 1.7259852886199951.\n",
            "Epoch 154 of 1000 took 7.368s\n",
            "The Discriminator loss is 1.3572816848754883 and the Generator loss is 1.7411693334579468.\n",
            "Epoch 155 of 1000 took 7.176s\n",
            "The Discriminator loss is 1.353430986404419 and the Generator loss is 3.6198883056640625.\n",
            "Epoch 156 of 1000 took 7.026s\n",
            "The Discriminator loss is 0.8769090175628662 and the Generator loss is 5.032360553741455.\n",
            "Epoch 157 of 1000 took 7.094s\n",
            "The Discriminator loss is 1.8383584022521973 and the Generator loss is 2.280573844909668.\n",
            "Epoch 158 of 1000 took 7.172s\n",
            "The Discriminator loss is 1.2001310586929321 and the Generator loss is 2.005906343460083.\n",
            "Epoch 159 of 1000 took 7.252s\n",
            "The Discriminator loss is 1.6391850709915161 and the Generator loss is 2.804938316345215.\n",
            "Epoch 160 of 1000 took 6.944s\n",
            "The Discriminator loss is 1.8186068534851074 and the Generator loss is 2.825613498687744.\n",
            "Epoch 161 of 1000 took 7.037s\n",
            "The Discriminator loss is 0.9281960725784302 and the Generator loss is 1.3918107748031616.\n",
            "Epoch 162 of 1000 took 6.982s\n",
            "The Discriminator loss is 1.267293930053711 and the Generator loss is 2.6372876167297363.\n",
            "Epoch 163 of 1000 took 6.899s\n",
            "The Discriminator loss is 1.2107019424438477 and the Generator loss is 2.1844441890716553.\n",
            "Epoch 164 of 1000 took 6.940s\n",
            "The Discriminator loss is 2.163701295852661 and the Generator loss is 1.5480304956436157.\n",
            "Epoch 165 of 1000 took 6.789s\n",
            "The Discriminator loss is 1.2670166492462158 and the Generator loss is 1.8023881912231445.\n",
            "Epoch 166 of 1000 took 6.847s\n",
            "The Discriminator loss is 1.439990758895874 and the Generator loss is 2.3728458881378174.\n",
            "Epoch 167 of 1000 took 7.068s\n",
            "The Discriminator loss is 0.721947431564331 and the Generator loss is 5.227461338043213.\n",
            "Epoch 168 of 1000 took 7.080s\n",
            "The Discriminator loss is 1.0151591300964355 and the Generator loss is 2.4532177448272705.\n",
            "Epoch 169 of 1000 took 7.116s\n",
            "The Discriminator loss is 1.4523128271102905 and the Generator loss is 5.388040542602539.\n",
            "Epoch 170 of 1000 took 7.047s\n",
            "The Discriminator loss is 1.5601880550384521 and the Generator loss is 2.243147850036621.\n",
            "Epoch 171 of 1000 took 7.139s\n",
            "The Discriminator loss is 0.7532832622528076 and the Generator loss is 6.0049943923950195.\n",
            "Epoch 172 of 1000 took 7.149s\n",
            "The Discriminator loss is 0.8481343388557434 and the Generator loss is 4.5826616287231445.\n",
            "Epoch 173 of 1000 took 7.155s\n",
            "The Discriminator loss is 0.9352823495864868 and the Generator loss is 4.856540679931641.\n",
            "Epoch 174 of 1000 took 6.973s\n",
            "The Discriminator loss is 1.1608643531799316 and the Generator loss is 1.7122877836227417.\n",
            "Epoch 175 of 1000 took 6.946s\n",
            "The Discriminator loss is 0.9688041806221008 and the Generator loss is 1.8585337400436401.\n",
            "Epoch 176 of 1000 took 7.143s\n",
            "The Discriminator loss is 1.5084888935089111 and the Generator loss is 1.6907039880752563.\n",
            "Epoch 177 of 1000 took 7.072s\n",
            "The Discriminator loss is 1.629286289215088 and the Generator loss is 1.616094946861267.\n",
            "Epoch 178 of 1000 took 7.087s\n",
            "The Discriminator loss is 1.0530469417572021 and the Generator loss is 2.6792919635772705.\n",
            "Epoch 179 of 1000 took 6.960s\n",
            "The Discriminator loss is 1.2671003341674805 and the Generator loss is 7.742236137390137.\n",
            "Epoch 180 of 1000 took 7.015s\n",
            "The Discriminator loss is 1.7022249698638916 and the Generator loss is 1.4512991905212402.\n",
            "Epoch 181 of 1000 took 7.040s\n",
            "The Discriminator loss is 1.2595195770263672 and the Generator loss is 3.3425498008728027.\n",
            "Epoch 182 of 1000 took 6.953s\n",
            "The Discriminator loss is 1.6239274740219116 and the Generator loss is 3.474665880203247.\n",
            "Epoch 183 of 1000 took 6.974s\n",
            "The Discriminator loss is 1.9819968938827515 and the Generator loss is 0.6812102794647217.\n",
            "Epoch 184 of 1000 took 6.923s\n",
            "The Discriminator loss is 1.421607255935669 and the Generator loss is 1.9705069065093994.\n",
            "Epoch 185 of 1000 took 6.976s\n",
            "The Discriminator loss is 1.020349144935608 and the Generator loss is 1.7156926393508911.\n",
            "Epoch 186 of 1000 took 7.226s\n",
            "The Discriminator loss is 2.102231025695801 and the Generator loss is 2.5384604930877686.\n",
            "Epoch 187 of 1000 took 7.331s\n",
            "The Discriminator loss is 0.979972779750824 and the Generator loss is 3.6655054092407227.\n",
            "Epoch 188 of 1000 took 7.433s\n",
            "The Discriminator loss is 1.4267126321792603 and the Generator loss is 1.3208074569702148.\n",
            "Epoch 189 of 1000 took 7.568s\n",
            "The Discriminator loss is 1.8848180770874023 and the Generator loss is 1.4436030387878418.\n",
            "Epoch 190 of 1000 took 7.498s\n",
            "The Discriminator loss is 0.9358310699462891 and the Generator loss is 2.8842520713806152.\n",
            "Epoch 191 of 1000 took 7.369s\n",
            "The Discriminator loss is 2.1732444763183594 and the Generator loss is 1.9730210304260254.\n",
            "Epoch 192 of 1000 took 7.534s\n",
            "The Discriminator loss is 1.020378589630127 and the Generator loss is 1.7134978771209717.\n",
            "Epoch 193 of 1000 took 7.653s\n",
            "The Discriminator loss is 1.5207394361495972 and the Generator loss is 2.211191177368164.\n",
            "Epoch 194 of 1000 took 7.616s\n",
            "The Discriminator loss is 1.1933979988098145 and the Generator loss is 3.195390224456787.\n",
            "Epoch 195 of 1000 took 7.587s\n",
            "The Discriminator loss is 1.610955834388733 and the Generator loss is 1.613214135169983.\n",
            "Epoch 196 of 1000 took 7.682s\n",
            "The Discriminator loss is 1.4021906852722168 and the Generator loss is 8.850357055664062.\n",
            "Epoch 197 of 1000 took 7.923s\n",
            "The Discriminator loss is 1.0353186130523682 and the Generator loss is 2.709418296813965.\n",
            "Epoch 198 of 1000 took 7.632s\n",
            "The Discriminator loss is 2.043738603591919 and the Generator loss is 1.6540327072143555.\n",
            "Epoch 199 of 1000 took 7.510s\n",
            "The Discriminator loss is 1.458242654800415 and the Generator loss is 1.5590760707855225.\n",
            "Epoch 200 of 1000 took 7.655s\n",
            "The Discriminator loss is 1.570957899093628 and the Generator loss is 1.4591423273086548.\n",
            "Epoch 201 of 1000 took 7.776s\n",
            "The Discriminator loss is 1.0896002054214478 and the Generator loss is 1.994381308555603.\n",
            "Epoch 202 of 1000 took 7.608s\n",
            "The Discriminator loss is 1.7596912384033203 and the Generator loss is 1.8117843866348267.\n",
            "Epoch 203 of 1000 took 7.505s\n",
            "The Discriminator loss is 1.6155260801315308 and the Generator loss is 0.8285852670669556.\n",
            "Epoch 204 of 1000 took 7.527s\n",
            "The Discriminator loss is 1.0774478912353516 and the Generator loss is 2.215790033340454.\n",
            "Epoch 205 of 1000 took 7.509s\n",
            "The Discriminator loss is 0.9246029853820801 and the Generator loss is 3.5243117809295654.\n",
            "Epoch 206 of 1000 took 7.440s\n",
            "The Discriminator loss is 1.864577054977417 and the Generator loss is 1.3287572860717773.\n",
            "Epoch 207 of 1000 took 7.263s\n",
            "The Discriminator loss is 1.8397834300994873 and the Generator loss is 1.7841126918792725.\n",
            "Epoch 208 of 1000 took 7.309s\n",
            "The Discriminator loss is 1.1872413158416748 and the Generator loss is 3.184429407119751.\n",
            "Epoch 209 of 1000 took 7.340s\n",
            "The Discriminator loss is 2.799330234527588 and the Generator loss is 4.791417598724365.\n",
            "Epoch 210 of 1000 took 7.239s\n",
            "The Discriminator loss is 1.5626981258392334 and the Generator loss is 1.7555131912231445.\n",
            "Epoch 211 of 1000 took 7.315s\n",
            "The Discriminator loss is 2.0341100692749023 and the Generator loss is 1.0376263856887817.\n",
            "Epoch 212 of 1000 took 7.340s\n",
            "The Discriminator loss is 1.351806640625 and the Generator loss is 2.0911147594451904.\n",
            "Epoch 213 of 1000 took 7.328s\n",
            "The Discriminator loss is 1.4372799396514893 and the Generator loss is 1.310304880142212.\n",
            "Epoch 214 of 1000 took 7.377s\n",
            "The Discriminator loss is 1.847969651222229 and the Generator loss is 1.3720793724060059.\n",
            "Epoch 215 of 1000 took 7.162s\n",
            "The Discriminator loss is 1.1231706142425537 and the Generator loss is 3.1787052154541016.\n",
            "Epoch 216 of 1000 took 7.041s\n",
            "The Discriminator loss is 1.2832767963409424 and the Generator loss is 1.3449918031692505.\n",
            "Epoch 217 of 1000 took 7.021s\n",
            "The Discriminator loss is 2.114417791366577 and the Generator loss is 0.9839610457420349.\n",
            "Epoch 218 of 1000 took 6.999s\n",
            "The Discriminator loss is 1.4038515090942383 and the Generator loss is 1.996724247932434.\n",
            "Epoch 219 of 1000 took 6.928s\n",
            "The Discriminator loss is 1.1828041076660156 and the Generator loss is 4.883613586425781.\n",
            "Epoch 220 of 1000 took 6.905s\n",
            "The Discriminator loss is 1.4963443279266357 and the Generator loss is 2.2702462673187256.\n",
            "Epoch 221 of 1000 took 6.974s\n",
            "The Discriminator loss is 1.4065954685211182 and the Generator loss is 2.3575398921966553.\n",
            "Epoch 222 of 1000 took 7.091s\n",
            "The Discriminator loss is 1.49945068359375 and the Generator loss is 1.3975356817245483.\n",
            "Epoch 223 of 1000 took 6.914s\n",
            "The Discriminator loss is 1.590363621711731 and the Generator loss is 2.649965763092041.\n",
            "Epoch 224 of 1000 took 6.925s\n",
            "The Discriminator loss is 2.9104127883911133 and the Generator loss is 1.5139710903167725.\n",
            "Epoch 225 of 1000 took 6.871s\n",
            "The Discriminator loss is 1.6469733715057373 and the Generator loss is 1.0764156579971313.\n",
            "Epoch 226 of 1000 took 6.854s\n",
            "The Discriminator loss is 0.9938429594039917 and the Generator loss is 1.6266103982925415.\n",
            "Epoch 227 of 1000 took 6.732s\n",
            "The Discriminator loss is 1.9663012027740479 and the Generator loss is 2.4205591678619385.\n",
            "Epoch 228 of 1000 took 6.895s\n",
            "The Discriminator loss is 2.425593376159668 and the Generator loss is 2.1290080547332764.\n",
            "Epoch 229 of 1000 took 6.936s\n",
            "The Discriminator loss is 1.5351862907409668 and the Generator loss is 2.0124456882476807.\n",
            "Epoch 230 of 1000 took 7.058s\n",
            "The Discriminator loss is 1.9292956590652466 and the Generator loss is 1.3263643980026245.\n",
            "Epoch 231 of 1000 took 7.565s\n",
            "The Discriminator loss is 1.8709208965301514 and the Generator loss is 1.580965518951416.\n",
            "Epoch 232 of 1000 took 7.590s\n",
            "The Discriminator loss is 1.0474060773849487 and the Generator loss is 1.611317753791809.\n",
            "Epoch 233 of 1000 took 7.544s\n",
            "The Discriminator loss is 0.799791693687439 and the Generator loss is 4.663739204406738.\n",
            "Epoch 234 of 1000 took 7.573s\n",
            "The Discriminator loss is 1.4135042428970337 and the Generator loss is 2.217960834503174.\n",
            "Epoch 235 of 1000 took 7.766s\n",
            "The Discriminator loss is 1.4876644611358643 and the Generator loss is 1.443914771080017.\n",
            "Epoch 236 of 1000 took 7.808s\n",
            "The Discriminator loss is 1.1285905838012695 and the Generator loss is 3.3016879558563232.\n",
            "Epoch 237 of 1000 took 7.667s\n",
            "The Discriminator loss is 1.0923762321472168 and the Generator loss is 1.4630106687545776.\n",
            "Epoch 238 of 1000 took 7.430s\n",
            "The Discriminator loss is 1.6814459562301636 and the Generator loss is 1.7209384441375732.\n",
            "Epoch 239 of 1000 took 7.604s\n",
            "The Discriminator loss is 1.0621790885925293 and the Generator loss is 2.7180635929107666.\n",
            "Epoch 240 of 1000 took 7.781s\n",
            "The Discriminator loss is 1.9887304306030273 and the Generator loss is 1.8669565916061401.\n",
            "Epoch 241 of 1000 took 7.377s\n",
            "The Discriminator loss is 1.4827702045440674 and the Generator loss is 2.3671135902404785.\n",
            "Epoch 242 of 1000 took 7.369s\n",
            "The Discriminator loss is 1.1348578929901123 and the Generator loss is 1.5165677070617676.\n",
            "Epoch 243 of 1000 took 7.554s\n",
            "The Discriminator loss is 1.503345251083374 and the Generator loss is 2.1933560371398926.\n",
            "Epoch 244 of 1000 took 7.473s\n",
            "The Discriminator loss is 2.522341012954712 and the Generator loss is 3.004164457321167.\n",
            "Epoch 245 of 1000 took 7.289s\n",
            "The Discriminator loss is 1.5040653944015503 and the Generator loss is 1.8930113315582275.\n",
            "Epoch 246 of 1000 took 7.264s\n",
            "The Discriminator loss is 1.0384762287139893 and the Generator loss is 1.7206059694290161.\n",
            "Epoch 247 of 1000 took 7.325s\n",
            "The Discriminator loss is 1.7564244270324707 and the Generator loss is 0.7514694929122925.\n",
            "Epoch 248 of 1000 took 7.299s\n",
            "The Discriminator loss is 1.2536506652832031 and the Generator loss is 3.430961847305298.\n",
            "Epoch 249 of 1000 took 7.289s\n",
            "The Discriminator loss is 1.7079302072525024 and the Generator loss is 5.068341255187988.\n",
            "Epoch 250 of 1000 took 7.241s\n",
            "The Discriminator loss is 2.2963478565216064 and the Generator loss is 2.712242841720581.\n",
            "Epoch 251 of 1000 took 7.224s\n",
            "The Discriminator loss is 2.08223819732666 and the Generator loss is 3.706960439682007.\n",
            "Epoch 252 of 1000 took 7.198s\n",
            "The Discriminator loss is 0.8200994729995728 and the Generator loss is 2.135115385055542.\n",
            "Epoch 253 of 1000 took 7.073s\n",
            "The Discriminator loss is 1.8724275827407837 and the Generator loss is 1.719069004058838.\n",
            "Epoch 254 of 1000 took 7.086s\n",
            "The Discriminator loss is 1.6701033115386963 and the Generator loss is 2.941819667816162.\n",
            "Epoch 255 of 1000 took 7.078s\n",
            "The Discriminator loss is 1.6803436279296875 and the Generator loss is 1.227213978767395.\n",
            "Epoch 256 of 1000 took 7.099s\n",
            "The Discriminator loss is 1.2742373943328857 and the Generator loss is 1.8867573738098145.\n",
            "Epoch 257 of 1000 took 7.021s\n",
            "The Discriminator loss is 1.620508074760437 and the Generator loss is 1.705517292022705.\n",
            "Epoch 258 of 1000 took 7.265s\n",
            "The Discriminator loss is 2.125486373901367 and the Generator loss is 2.283707857131958.\n",
            "Epoch 259 of 1000 took 7.304s\n",
            "The Discriminator loss is 0.7295598387718201 and the Generator loss is 7.917787075042725.\n",
            "Epoch 260 of 1000 took 7.264s\n",
            "The Discriminator loss is 1.8093528747558594 and the Generator loss is 7.720941066741943.\n",
            "Epoch 261 of 1000 took 7.340s\n",
            "The Discriminator loss is 1.8703056573867798 and the Generator loss is 2.6183083057403564.\n",
            "Epoch 262 of 1000 took 7.120s\n",
            "The Discriminator loss is 1.3812854290008545 and the Generator loss is 2.159991502761841.\n",
            "Epoch 263 of 1000 took 7.046s\n",
            "The Discriminator loss is 1.5399247407913208 and the Generator loss is 1.2160285711288452.\n",
            "Epoch 264 of 1000 took 7.240s\n",
            "The Discriminator loss is 2.6794071197509766 and the Generator loss is 0.9733107686042786.\n",
            "Epoch 265 of 1000 took 7.300s\n",
            "The Discriminator loss is 1.6856975555419922 and the Generator loss is 0.9707875847816467.\n",
            "Epoch 266 of 1000 took 7.105s\n",
            "The Discriminator loss is 1.8335492610931396 and the Generator loss is 4.871219635009766.\n",
            "Epoch 267 of 1000 took 7.122s\n",
            "The Discriminator loss is 1.6940717697143555 and the Generator loss is 3.1992597579956055.\n",
            "Epoch 268 of 1000 took 7.174s\n",
            "The Discriminator loss is 1.3482744693756104 and the Generator loss is 1.562179684638977.\n",
            "Epoch 269 of 1000 took 7.094s\n",
            "The Discriminator loss is 1.4726232290267944 and the Generator loss is 0.9289041757583618.\n",
            "Epoch 270 of 1000 took 7.040s\n",
            "The Discriminator loss is 2.5063905715942383 and the Generator loss is 1.5111792087554932.\n",
            "Epoch 271 of 1000 took 7.001s\n",
            "The Discriminator loss is 1.3062443733215332 and the Generator loss is 1.5877957344055176.\n",
            "Epoch 272 of 1000 took 7.007s\n",
            "The Discriminator loss is 1.7319035530090332 and the Generator loss is 1.2713541984558105.\n",
            "Epoch 273 of 1000 took 7.011s\n",
            "The Discriminator loss is 1.5903379917144775 and the Generator loss is 2.4358110427856445.\n",
            "Epoch 274 of 1000 took 6.974s\n",
            "The Discriminator loss is 1.019553780555725 and the Generator loss is 1.7726294994354248.\n",
            "Epoch 275 of 1000 took 7.014s\n",
            "The Discriminator loss is 1.5097236633300781 and the Generator loss is 1.8147779703140259.\n",
            "Epoch 276 of 1000 took 6.950s\n",
            "The Discriminator loss is 2.1441853046417236 and the Generator loss is 2.6232450008392334.\n",
            "Epoch 277 of 1000 took 6.995s\n",
            "The Discriminator loss is 0.9618997573852539 and the Generator loss is 5.566804885864258.\n",
            "Epoch 278 of 1000 took 7.037s\n",
            "The Discriminator loss is 1.9076242446899414 and the Generator loss is 8.299277305603027.\n",
            "Epoch 279 of 1000 took 7.036s\n",
            "The Discriminator loss is 1.5377850532531738 and the Generator loss is 1.854803442955017.\n",
            "Epoch 280 of 1000 took 7.047s\n",
            "The Discriminator loss is 1.4698457717895508 and the Generator loss is 2.4604923725128174.\n",
            "Epoch 281 of 1000 took 7.100s\n",
            "The Discriminator loss is 1.144650936126709 and the Generator loss is 3.8647518157958984.\n",
            "Epoch 282 of 1000 took 7.409s\n",
            "The Discriminator loss is 2.0412516593933105 and the Generator loss is 1.3951889276504517.\n",
            "Epoch 283 of 1000 took 7.656s\n",
            "The Discriminator loss is 1.3024914264678955 and the Generator loss is 1.119328260421753.\n",
            "Epoch 284 of 1000 took 7.427s\n",
            "The Discriminator loss is 2.0606954097747803 and the Generator loss is 1.3866653442382812.\n",
            "Epoch 285 of 1000 took 7.320s\n",
            "The Discriminator loss is 1.2290997505187988 and the Generator loss is 2.8389246463775635.\n",
            "Epoch 286 of 1000 took 7.523s\n",
            "The Discriminator loss is 2.102318286895752 and the Generator loss is 1.455672264099121.\n",
            "Epoch 287 of 1000 took 7.433s\n",
            "The Discriminator loss is 2.4849352836608887 and the Generator loss is 2.0304129123687744.\n",
            "Epoch 288 of 1000 took 7.365s\n",
            "The Discriminator loss is 2.011307716369629 and the Generator loss is 1.5269749164581299.\n",
            "Epoch 289 of 1000 took 7.388s\n",
            "The Discriminator loss is 1.7534918785095215 and the Generator loss is 1.3512970209121704.\n",
            "Epoch 290 of 1000 took 7.442s\n",
            "The Discriminator loss is 1.0920915603637695 and the Generator loss is 3.5164291858673096.\n",
            "Epoch 291 of 1000 took 7.558s\n",
            "The Discriminator loss is 2.154790163040161 and the Generator loss is 2.101829767227173.\n",
            "Epoch 292 of 1000 took 7.592s\n",
            "The Discriminator loss is 0.6727619171142578 and the Generator loss is 5.772162914276123.\n",
            "Epoch 293 of 1000 took 7.682s\n",
            "The Discriminator loss is 0.9876390099525452 and the Generator loss is 3.2578067779541016.\n",
            "Epoch 294 of 1000 took 7.654s\n",
            "The Discriminator loss is 2.4612340927124023 and the Generator loss is 1.289894461631775.\n",
            "Epoch 295 of 1000 took 7.451s\n",
            "The Discriminator loss is 2.2491650581359863 and the Generator loss is 2.029650926589966.\n",
            "Epoch 296 of 1000 took 7.298s\n",
            "The Discriminator loss is 1.9291020631790161 and the Generator loss is 9.506671905517578.\n",
            "Epoch 297 of 1000 took 7.354s\n",
            "The Discriminator loss is 0.9594442844390869 and the Generator loss is 1.732688307762146.\n",
            "Epoch 298 of 1000 took 7.221s\n",
            "The Discriminator loss is 1.3699861764907837 and the Generator loss is 1.3064221143722534.\n",
            "Epoch 299 of 1000 took 7.174s\n",
            "The Discriminator loss is 2.152876138687134 and the Generator loss is 1.1025153398513794.\n",
            "Epoch 300 of 1000 took 7.350s\n",
            "The Discriminator loss is 2.419546604156494 and the Generator loss is 0.9459652900695801.\n",
            "Epoch 301 of 1000 took 7.360s\n",
            "The Discriminator loss is 1.7129895687103271 and the Generator loss is 0.8152040839195251.\n",
            "Epoch 302 of 1000 took 7.336s\n",
            "The Discriminator loss is 1.934274673461914 and the Generator loss is 1.171994686126709.\n",
            "Epoch 303 of 1000 took 7.085s\n",
            "The Discriminator loss is 1.1987022161483765 and the Generator loss is 1.7770166397094727.\n",
            "Epoch 304 of 1000 took 7.092s\n",
            "The Discriminator loss is 1.905319094657898 and the Generator loss is 1.9774229526519775.\n",
            "Epoch 305 of 1000 took 7.092s\n",
            "The Discriminator loss is 1.3886115550994873 and the Generator loss is 3.5034778118133545.\n",
            "Epoch 306 of 1000 took 6.992s\n",
            "The Discriminator loss is 2.085723400115967 and the Generator loss is 1.6993523836135864.\n",
            "Epoch 307 of 1000 took 7.134s\n",
            "The Discriminator loss is 1.0801185369491577 and the Generator loss is 2.4428110122680664.\n",
            "Epoch 308 of 1000 took 7.287s\n",
            "The Discriminator loss is 1.6493779420852661 and the Generator loss is 1.0974345207214355.\n",
            "Epoch 309 of 1000 took 7.234s\n",
            "The Discriminator loss is 1.6593503952026367 and the Generator loss is 1.977932095527649.\n",
            "Epoch 310 of 1000 took 7.210s\n",
            "The Discriminator loss is 1.7826366424560547 and the Generator loss is 1.2283108234405518.\n",
            "Epoch 311 of 1000 took 7.184s\n",
            "The Discriminator loss is 1.8745782375335693 and the Generator loss is 2.2476322650909424.\n",
            "Epoch 312 of 1000 took 7.042s\n",
            "The Discriminator loss is 1.0909875631332397 and the Generator loss is 2.068802833557129.\n",
            "Epoch 313 of 1000 took 7.060s\n",
            "The Discriminator loss is 1.6306278705596924 and the Generator loss is 1.6586915254592896.\n",
            "Epoch 314 of 1000 took 7.081s\n",
            "The Discriminator loss is 0.8711130619049072 and the Generator loss is 1.8464813232421875.\n",
            "Epoch 315 of 1000 took 7.130s\n",
            "The Discriminator loss is 2.5868921279907227 and the Generator loss is 1.5349757671356201.\n",
            "Epoch 316 of 1000 took 7.133s\n",
            "The Discriminator loss is 1.5973279476165771 and the Generator loss is 2.381225109100342.\n",
            "Epoch 317 of 1000 took 7.147s\n",
            "The Discriminator loss is 2.0187506675720215 and the Generator loss is 1.58038330078125.\n",
            "Epoch 318 of 1000 took 7.079s\n",
            "The Discriminator loss is 1.4694223403930664 and the Generator loss is 1.5676815509796143.\n",
            "Epoch 319 of 1000 took 7.085s\n",
            "The Discriminator loss is 1.6375226974487305 and the Generator loss is 2.183790922164917.\n",
            "Epoch 320 of 1000 took 7.063s\n",
            "The Discriminator loss is 1.609501838684082 and the Generator loss is 8.518011093139648.\n",
            "Epoch 321 of 1000 took 7.081s\n",
            "The Discriminator loss is 1.6184358596801758 and the Generator loss is 1.9157201051712036.\n",
            "Epoch 322 of 1000 took 7.058s\n",
            "The Discriminator loss is 2.9255447387695312 and the Generator loss is 3.845118999481201.\n",
            "Epoch 323 of 1000 took 7.128s\n",
            "The Discriminator loss is 1.2693763971328735 and the Generator loss is 1.4614710807800293.\n",
            "Epoch 324 of 1000 took 7.009s\n",
            "The Discriminator loss is 1.5392391681671143 and the Generator loss is 1.5217660665512085.\n",
            "Epoch 325 of 1000 took 7.086s\n",
            "The Discriminator loss is 2.359018325805664 and the Generator loss is 1.746587872505188.\n",
            "Epoch 326 of 1000 took 7.383s\n",
            "The Discriminator loss is 1.5097604990005493 and the Generator loss is 1.1306344270706177.\n",
            "Epoch 327 of 1000 took 7.185s\n",
            "The Discriminator loss is 1.1857634782791138 and the Generator loss is 1.1502065658569336.\n",
            "Epoch 328 of 1000 took 7.088s\n",
            "The Discriminator loss is 1.796177864074707 and the Generator loss is 1.2381329536437988.\n",
            "Epoch 329 of 1000 took 7.378s\n",
            "The Discriminator loss is 2.356268882751465 and the Generator loss is 1.2043136358261108.\n",
            "Epoch 330 of 1000 took 7.133s\n",
            "The Discriminator loss is 1.9306437969207764 and the Generator loss is 0.9386348128318787.\n",
            "Epoch 331 of 1000 took 7.142s\n",
            "The Discriminator loss is 1.9993987083435059 and the Generator loss is 2.3775415420532227.\n",
            "Epoch 332 of 1000 took 7.126s\n",
            "The Discriminator loss is 1.6450004577636719 and the Generator loss is 0.9034432768821716.\n",
            "Epoch 333 of 1000 took 7.495s\n",
            "The Discriminator loss is 2.321629524230957 and the Generator loss is 0.7246027588844299.\n",
            "Epoch 334 of 1000 took 7.454s\n",
            "The Discriminator loss is 1.4633526802062988 and the Generator loss is 1.1600677967071533.\n",
            "Epoch 335 of 1000 took 7.461s\n",
            "The Discriminator loss is 1.9373184442520142 and the Generator loss is 1.2606253623962402.\n",
            "Epoch 336 of 1000 took 7.566s\n",
            "The Discriminator loss is 1.3380268812179565 and the Generator loss is 2.070448875427246.\n",
            "Epoch 337 of 1000 took 7.496s\n",
            "The Discriminator loss is 1.5705358982086182 and the Generator loss is 1.8280950784683228.\n",
            "Epoch 338 of 1000 took 7.451s\n",
            "The Discriminator loss is 2.000070571899414 and the Generator loss is 0.7949303388595581.\n",
            "Epoch 339 of 1000 took 7.422s\n",
            "The Discriminator loss is 1.6234605312347412 and the Generator loss is 1.1535426378250122.\n",
            "Epoch 340 of 1000 took 7.427s\n",
            "The Discriminator loss is 1.3910589218139648 and the Generator loss is 3.313394784927368.\n",
            "Epoch 341 of 1000 took 7.447s\n",
            "The Discriminator loss is 1.3521472215652466 and the Generator loss is 1.509594440460205.\n",
            "Epoch 342 of 1000 took 7.371s\n",
            "The Discriminator loss is 0.6890348196029663 and the Generator loss is 2.797130823135376.\n",
            "Epoch 343 of 1000 took 7.311s\n",
            "The Discriminator loss is 1.575727939605713 and the Generator loss is 0.8197606801986694.\n",
            "Epoch 344 of 1000 took 7.366s\n",
            "The Discriminator loss is 0.6337325572967529 and the Generator loss is 2.876304864883423.\n",
            "Epoch 345 of 1000 took 7.275s\n",
            "The Discriminator loss is 1.7815883159637451 and the Generator loss is 1.459279179573059.\n",
            "Epoch 346 of 1000 took 7.233s\n",
            "The Discriminator loss is 1.6562128067016602 and the Generator loss is 1.898646593093872.\n",
            "Epoch 347 of 1000 took 7.269s\n",
            "The Discriminator loss is 1.9032244682312012 and the Generator loss is 2.090437412261963.\n",
            "Epoch 348 of 1000 took 7.182s\n",
            "The Discriminator loss is 2.1488380432128906 and the Generator loss is 1.0793439149856567.\n",
            "Epoch 349 of 1000 took 7.030s\n",
            "The Discriminator loss is 1.2954895496368408 and the Generator loss is 3.056668281555176.\n",
            "Epoch 350 of 1000 took 7.063s\n",
            "The Discriminator loss is 2.306652069091797 and the Generator loss is 0.9348970651626587.\n",
            "Epoch 351 of 1000 took 7.030s\n",
            "The Discriminator loss is 1.5496928691864014 and the Generator loss is 2.178123712539673.\n",
            "Epoch 352 of 1000 took 7.083s\n",
            "The Discriminator loss is 2.500425338745117 and the Generator loss is 2.0910441875457764.\n",
            "Epoch 353 of 1000 took 7.102s\n",
            "The Discriminator loss is 1.8760614395141602 and the Generator loss is 1.3292125463485718.\n",
            "Epoch 354 of 1000 took 7.064s\n",
            "The Discriminator loss is 1.2789499759674072 and the Generator loss is 1.154545783996582.\n",
            "Epoch 355 of 1000 took 7.101s\n",
            "The Discriminator loss is 1.9775056838989258 and the Generator loss is 6.312430381774902.\n",
            "Epoch 356 of 1000 took 7.047s\n",
            "The Discriminator loss is 1.550832986831665 and the Generator loss is 1.0464560985565186.\n",
            "Epoch 357 of 1000 took 7.167s\n",
            "The Discriminator loss is 2.670699119567871 and the Generator loss is 1.8165494203567505.\n",
            "Epoch 358 of 1000 took 7.170s\n",
            "The Discriminator loss is 2.0840601921081543 and the Generator loss is 0.7804146409034729.\n",
            "Epoch 359 of 1000 took 7.094s\n",
            "The Discriminator loss is 0.538432776927948 and the Generator loss is 3.665349006652832.\n",
            "Epoch 360 of 1000 took 7.175s\n",
            "The Discriminator loss is 3.127737045288086 and the Generator loss is 1.809343695640564.\n",
            "Epoch 361 of 1000 took 7.390s\n",
            "The Discriminator loss is 0.5195103287696838 and the Generator loss is 5.374811172485352.\n",
            "Epoch 362 of 1000 took 7.392s\n",
            "The Discriminator loss is 1.7794737815856934 and the Generator loss is 0.9730637073516846.\n",
            "Epoch 363 of 1000 took 7.303s\n",
            "The Discriminator loss is 1.9343032836914062 and the Generator loss is 0.7516847252845764.\n",
            "Epoch 364 of 1000 took 7.261s\n",
            "The Discriminator loss is 2.7944040298461914 and the Generator loss is 0.9127237200737.\n",
            "Epoch 365 of 1000 took 7.120s\n",
            "The Discriminator loss is 1.5785892009735107 and the Generator loss is 0.994384765625.\n",
            "Epoch 366 of 1000 took 7.043s\n",
            "The Discriminator loss is 2.4017186164855957 and the Generator loss is 1.6218440532684326.\n",
            "Epoch 367 of 1000 took 7.230s\n",
            "The Discriminator loss is 1.9981465339660645 and the Generator loss is 0.9678819179534912.\n",
            "Epoch 368 of 1000 took 7.340s\n",
            "The Discriminator loss is 1.6720118522644043 and the Generator loss is 1.4858561754226685.\n",
            "Epoch 369 of 1000 took 7.548s\n",
            "The Discriminator loss is 2.304903507232666 and the Generator loss is 1.0801371335983276.\n",
            "Epoch 370 of 1000 took 7.320s\n",
            "The Discriminator loss is 2.0036582946777344 and the Generator loss is 5.438546180725098.\n",
            "Epoch 371 of 1000 took 7.453s\n",
            "The Discriminator loss is 1.3951897621154785 and the Generator loss is 1.0868104696273804.\n",
            "Epoch 372 of 1000 took 7.286s\n",
            "The Discriminator loss is 1.7263445854187012 and the Generator loss is 2.265092372894287.\n",
            "Epoch 373 of 1000 took 7.196s\n",
            "The Discriminator loss is 1.0029700994491577 and the Generator loss is 1.250432014465332.\n",
            "Epoch 374 of 1000 took 7.275s\n",
            "The Discriminator loss is 1.8761498928070068 and the Generator loss is 1.2955617904663086.\n",
            "Epoch 375 of 1000 took 7.261s\n",
            "The Discriminator loss is 0.8849695920944214 and the Generator loss is 2.447357177734375.\n",
            "Epoch 376 of 1000 took 7.225s\n",
            "The Discriminator loss is 1.030545711517334 and the Generator loss is 1.7424533367156982.\n",
            "Epoch 377 of 1000 took 7.323s\n",
            "The Discriminator loss is 1.3186912536621094 and the Generator loss is 2.5148093700408936.\n",
            "Epoch 378 of 1000 took 7.281s\n",
            "The Discriminator loss is 1.295827865600586 and the Generator loss is 3.771674871444702.\n",
            "Epoch 379 of 1000 took 7.277s\n",
            "The Discriminator loss is 4.677914142608643 and the Generator loss is 1.502996802330017.\n",
            "Epoch 380 of 1000 took 7.298s\n",
            "The Discriminator loss is 1.452958583831787 and the Generator loss is 1.8074924945831299.\n",
            "Epoch 381 of 1000 took 7.358s\n",
            "The Discriminator loss is 1.2459814548492432 and the Generator loss is 4.621053695678711.\n",
            "Epoch 382 of 1000 took 7.365s\n",
            "The Discriminator loss is 1.0792505741119385 and the Generator loss is 1.244528889656067.\n",
            "Epoch 383 of 1000 took 7.342s\n",
            "The Discriminator loss is 2.4548139572143555 and the Generator loss is 1.963470458984375.\n",
            "Epoch 384 of 1000 took 7.292s\n",
            "The Discriminator loss is 1.8281443119049072 and the Generator loss is 0.8568481206893921.\n",
            "Epoch 385 of 1000 took 7.230s\n",
            "The Discriminator loss is 1.7855567932128906 and the Generator loss is 1.7310822010040283.\n",
            "Epoch 386 of 1000 took 7.304s\n",
            "The Discriminator loss is 1.7622976303100586 and the Generator loss is 1.2981581687927246.\n",
            "Epoch 387 of 1000 took 7.350s\n",
            "The Discriminator loss is 1.215636968612671 and the Generator loss is 1.8456820249557495.\n",
            "Epoch 388 of 1000 took 7.332s\n",
            "The Discriminator loss is 1.526538372039795 and the Generator loss is 0.9028639197349548.\n",
            "Epoch 389 of 1000 took 7.358s\n",
            "The Discriminator loss is 1.9283967018127441 and the Generator loss is 1.6855255365371704.\n",
            "Epoch 390 of 1000 took 7.208s\n",
            "The Discriminator loss is 2.4219627380371094 and the Generator loss is 1.8233892917633057.\n",
            "Epoch 391 of 1000 took 7.160s\n",
            "The Discriminator loss is 1.7225244045257568 and the Generator loss is 4.699758529663086.\n",
            "Epoch 392 of 1000 took 7.163s\n",
            "The Discriminator loss is 1.5638737678527832 and the Generator loss is 1.304591417312622.\n",
            "Epoch 393 of 1000 took 7.075s\n",
            "The Discriminator loss is 1.882840871810913 and the Generator loss is 0.9380719065666199.\n",
            "Epoch 394 of 1000 took 7.148s\n",
            "The Discriminator loss is 1.9557950496673584 and the Generator loss is 1.3289964199066162.\n",
            "Epoch 395 of 1000 took 7.002s\n",
            "The Discriminator loss is 2.4671032428741455 and the Generator loss is 1.0281776189804077.\n",
            "Epoch 396 of 1000 took 7.105s\n",
            "The Discriminator loss is 1.4847347736358643 and the Generator loss is 2.1832399368286133.\n",
            "Epoch 397 of 1000 took 7.012s\n",
            "The Discriminator loss is 2.6779427528381348 and the Generator loss is 0.5132083892822266.\n",
            "Epoch 398 of 1000 took 7.027s\n",
            "The Discriminator loss is 1.8310303688049316 and the Generator loss is 1.2241184711456299.\n",
            "Epoch 399 of 1000 took 7.042s\n",
            "The Discriminator loss is 0.9107314348220825 and the Generator loss is 1.636976957321167.\n",
            "Epoch 400 of 1000 took 7.083s\n",
            "The Discriminator loss is 0.5819187164306641 and the Generator loss is 6.641368389129639.\n",
            "Epoch 401 of 1000 took 7.187s\n",
            "The Discriminator loss is 2.186358690261841 and the Generator loss is 1.1075319051742554.\n",
            "Now Images are being generated\n",
            "Epoch 402 of 1000 took 7.294s\n",
            "The Discriminator loss is 2.3937439918518066 and the Generator loss is 1.7897392511367798.\n",
            "Epoch 403 of 1000 took 7.379s\n",
            "The Discriminator loss is 1.7322443723678589 and the Generator loss is 1.2204995155334473.\n",
            "Epoch 404 of 1000 took 7.365s\n",
            "The Discriminator loss is 0.930823564529419 and the Generator loss is 3.166126012802124.\n",
            "Epoch 405 of 1000 took 7.231s\n",
            "The Discriminator loss is 1.196207046508789 and the Generator loss is 2.129230260848999.\n",
            "Epoch 406 of 1000 took 7.168s\n",
            "The Discriminator loss is 1.2735583782196045 and the Generator loss is 1.3388856649398804.\n",
            "Epoch 407 of 1000 took 7.226s\n",
            "The Discriminator loss is 1.6136817932128906 and the Generator loss is 0.8038232326507568.\n",
            "Epoch 408 of 1000 took 7.241s\n",
            "The Discriminator loss is 2.819912910461426 and the Generator loss is 2.2110178470611572.\n",
            "Epoch 409 of 1000 took 7.138s\n",
            "The Discriminator loss is 2.2458643913269043 and the Generator loss is 0.6995301246643066.\n",
            "Epoch 410 of 1000 took 7.297s\n",
            "The Discriminator loss is 1.0393826961517334 and the Generator loss is 4.821713447570801.\n",
            "Epoch 411 of 1000 took 7.487s\n",
            "The Discriminator loss is 1.4924793243408203 and the Generator loss is 1.5786705017089844.\n",
            "Epoch 412 of 1000 took 7.705s\n",
            "The Discriminator loss is 1.1873431205749512 and the Generator loss is 2.5421411991119385.\n",
            "Epoch 413 of 1000 took 7.447s\n",
            "The Discriminator loss is 0.7437422275543213 and the Generator loss is 2.118542432785034.\n",
            "Epoch 414 of 1000 took 7.534s\n",
            "The Discriminator loss is 1.39298677444458 and the Generator loss is 1.0466779470443726.\n",
            "Epoch 415 of 1000 took 7.231s\n",
            "The Discriminator loss is 1.742664098739624 and the Generator loss is 1.996886134147644.\n",
            "Epoch 416 of 1000 took 7.156s\n",
            "The Discriminator loss is 3.190476894378662 and the Generator loss is 1.0249167680740356.\n",
            "Epoch 417 of 1000 took 7.090s\n",
            "The Discriminator loss is 0.8694521188735962 and the Generator loss is 5.23094367980957.\n",
            "Epoch 418 of 1000 took 7.211s\n",
            "The Discriminator loss is 2.84849214553833 and the Generator loss is 1.1565707921981812.\n",
            "Epoch 419 of 1000 took 7.124s\n",
            "The Discriminator loss is 1.7402454614639282 and the Generator loss is 2.1528451442718506.\n",
            "Epoch 420 of 1000 took 7.149s\n",
            "The Discriminator loss is 1.390195608139038 and the Generator loss is 2.1276209354400635.\n",
            "Epoch 421 of 1000 took 7.146s\n",
            "The Discriminator loss is 1.1563955545425415 and the Generator loss is 1.1011923551559448.\n",
            "Epoch 422 of 1000 took 7.100s\n",
            "The Discriminator loss is 2.8705332279205322 and the Generator loss is 1.2822091579437256.\n",
            "Epoch 423 of 1000 took 7.129s\n",
            "The Discriminator loss is 0.8805649876594543 and the Generator loss is 3.756437063217163.\n",
            "Epoch 424 of 1000 took 7.168s\n",
            "The Discriminator loss is 2.5757851600646973 and the Generator loss is 2.277625322341919.\n",
            "Epoch 425 of 1000 took 7.072s\n",
            "The Discriminator loss is 1.9062349796295166 and the Generator loss is 0.9269473552703857.\n",
            "Epoch 426 of 1000 took 7.116s\n",
            "The Discriminator loss is 1.3496772050857544 and the Generator loss is 0.9595958590507507.\n",
            "Epoch 427 of 1000 took 7.124s\n",
            "The Discriminator loss is 2.8233184814453125 and the Generator loss is 1.4660917520523071.\n",
            "Epoch 428 of 1000 took 7.129s\n",
            "The Discriminator loss is 3.1168406009674072 and the Generator loss is 1.2803257703781128.\n",
            "Epoch 429 of 1000 took 7.065s\n",
            "The Discriminator loss is 0.5715747475624084 and the Generator loss is 3.7119598388671875.\n",
            "Epoch 430 of 1000 took 7.109s\n",
            "The Discriminator loss is 3.288384437561035 and the Generator loss is 1.9424799680709839.\n",
            "Epoch 431 of 1000 took 7.141s\n",
            "The Discriminator loss is 1.7325695753097534 and the Generator loss is 2.124885320663452.\n",
            "Epoch 432 of 1000 took 6.934s\n",
            "The Discriminator loss is 1.171830415725708 and the Generator loss is 3.70497989654541.\n",
            "Epoch 433 of 1000 took 7.160s\n",
            "The Discriminator loss is 2.707066535949707 and the Generator loss is 1.7799136638641357.\n",
            "Epoch 434 of 1000 took 7.356s\n",
            "The Discriminator loss is 2.785970687866211 and the Generator loss is 2.31299090385437.\n",
            "Epoch 435 of 1000 took 7.324s\n",
            "The Discriminator loss is 1.5485024452209473 and the Generator loss is 0.9344407916069031.\n",
            "Epoch 436 of 1000 took 7.307s\n",
            "The Discriminator loss is 0.7975693941116333 and the Generator loss is 1.6976721286773682.\n",
            "Epoch 437 of 1000 took 7.287s\n",
            "The Discriminator loss is 1.9594738483428955 and the Generator loss is 1.1638901233673096.\n",
            "Epoch 438 of 1000 took 7.396s\n",
            "The Discriminator loss is 1.9221069812774658 and the Generator loss is 1.0486538410186768.\n",
            "Epoch 439 of 1000 took 7.347s\n",
            "The Discriminator loss is 3.033997058868408 and the Generator loss is 5.825683116912842.\n",
            "Epoch 440 of 1000 took 7.105s\n",
            "The Discriminator loss is 1.3609287738800049 and the Generator loss is 2.5135233402252197.\n",
            "Epoch 441 of 1000 took 7.162s\n",
            "The Discriminator loss is 4.8956732749938965 and the Generator loss is 2.593710422515869.\n",
            "Epoch 442 of 1000 took 7.290s\n",
            "The Discriminator loss is 3.0969552993774414 and the Generator loss is 0.8379941582679749.\n",
            "Epoch 443 of 1000 took 7.244s\n",
            "The Discriminator loss is 1.789482831954956 and the Generator loss is 0.7163524031639099.\n",
            "Epoch 444 of 1000 took 7.098s\n",
            "The Discriminator loss is 2.2423195838928223 and the Generator loss is 2.224540948867798.\n",
            "Epoch 445 of 1000 took 7.172s\n",
            "The Discriminator loss is 1.8926317691802979 and the Generator loss is 6.8854570388793945.\n",
            "Epoch 446 of 1000 took 7.024s\n",
            "The Discriminator loss is 3.097573757171631 and the Generator loss is 1.6535301208496094.\n",
            "Epoch 447 of 1000 took 6.935s\n",
            "The Discriminator loss is 1.420259714126587 and the Generator loss is 1.6003203392028809.\n",
            "Epoch 448 of 1000 took 7.021s\n",
            "The Discriminator loss is 0.6757335066795349 and the Generator loss is 2.5187337398529053.\n",
            "Epoch 449 of 1000 took 6.970s\n",
            "The Discriminator loss is 1.9311237335205078 and the Generator loss is 1.435157299041748.\n",
            "Epoch 450 of 1000 took 7.043s\n",
            "The Discriminator loss is 2.4475655555725098 and the Generator loss is 1.3394863605499268.\n",
            "Epoch 451 of 1000 took 6.989s\n",
            "The Discriminator loss is 2.4897584915161133 and the Generator loss is 1.2935905456542969.\n",
            "Epoch 452 of 1000 took 7.043s\n",
            "The Discriminator loss is 1.1337285041809082 and the Generator loss is 1.5868093967437744.\n",
            "Epoch 453 of 1000 took 6.986s\n",
            "The Discriminator loss is 1.1712284088134766 and the Generator loss is 2.9833872318267822.\n",
            "Epoch 454 of 1000 took 6.990s\n",
            "The Discriminator loss is 0.7154804468154907 and the Generator loss is 2.741795539855957.\n",
            "Epoch 455 of 1000 took 7.234s\n",
            "The Discriminator loss is 1.495115876197815 and the Generator loss is 1.5284534692764282.\n",
            "Epoch 456 of 1000 took 7.244s\n",
            "The Discriminator loss is 1.5339105129241943 and the Generator loss is 5.6605706214904785.\n",
            "Epoch 457 of 1000 took 7.547s\n",
            "The Discriminator loss is 1.2871447801589966 and the Generator loss is 1.3396086692810059.\n",
            "Epoch 458 of 1000 took 7.333s\n",
            "The Discriminator loss is 2.2188234329223633 and the Generator loss is 1.1152606010437012.\n",
            "Epoch 459 of 1000 took 7.319s\n",
            "The Discriminator loss is 1.8590087890625 and the Generator loss is 0.8651875853538513.\n",
            "Epoch 460 of 1000 took 7.356s\n",
            "The Discriminator loss is 1.7354261875152588 and the Generator loss is 0.9665848016738892.\n",
            "Epoch 461 of 1000 took 7.336s\n",
            "The Discriminator loss is 1.653824806213379 and the Generator loss is 2.8459584712982178.\n",
            "Epoch 462 of 1000 took 7.308s\n",
            "The Discriminator loss is 0.9966892004013062 and the Generator loss is 2.0082101821899414.\n",
            "Epoch 463 of 1000 took 7.323s\n",
            "The Discriminator loss is 1.2145488262176514 and the Generator loss is 1.268274188041687.\n",
            "Epoch 464 of 1000 took 7.259s\n",
            "The Discriminator loss is 1.0255157947540283 and the Generator loss is 3.009641647338867.\n",
            "Epoch 465 of 1000 took 7.268s\n",
            "The Discriminator loss is 1.8373910188674927 and the Generator loss is 1.4935989379882812.\n",
            "Epoch 466 of 1000 took 7.284s\n",
            "The Discriminator loss is 2.6417465209960938 and the Generator loss is 0.6396173238754272.\n",
            "Epoch 467 of 1000 took 7.337s\n",
            "The Discriminator loss is 1.415910243988037 and the Generator loss is 2.2384657859802246.\n",
            "Epoch 468 of 1000 took 7.328s\n",
            "The Discriminator loss is 1.5948522090911865 and the Generator loss is 1.731896996498108.\n",
            "Epoch 469 of 1000 took 7.297s\n",
            "The Discriminator loss is 2.52148175239563 and the Generator loss is 2.3995471000671387.\n",
            "Epoch 470 of 1000 took 7.225s\n",
            "The Discriminator loss is 1.7256522178649902 and the Generator loss is 2.2728030681610107.\n",
            "Epoch 471 of 1000 took 7.199s\n",
            "The Discriminator loss is 2.172314167022705 and the Generator loss is 3.093766450881958.\n",
            "Epoch 472 of 1000 took 7.261s\n",
            "The Discriminator loss is 1.2630892992019653 and the Generator loss is 1.4862943887710571.\n",
            "Epoch 473 of 1000 took 7.263s\n",
            "The Discriminator loss is 2.969205856323242 and the Generator loss is 1.7094719409942627.\n",
            "Epoch 474 of 1000 took 7.269s\n",
            "The Discriminator loss is 1.6404969692230225 and the Generator loss is 1.3101269006729126.\n",
            "Epoch 475 of 1000 took 7.261s\n",
            "The Discriminator loss is 1.2833020687103271 and the Generator loss is 1.9556025266647339.\n",
            "Epoch 476 of 1000 took 7.243s\n",
            "The Discriminator loss is 2.222475290298462 and the Generator loss is 0.7979865074157715.\n",
            "Epoch 477 of 1000 took 7.231s\n",
            "The Discriminator loss is 1.7531449794769287 and the Generator loss is 1.1109285354614258.\n",
            "Epoch 478 of 1000 took 7.261s\n",
            "The Discriminator loss is 1.935813546180725 and the Generator loss is 1.3303598165512085.\n",
            "Epoch 479 of 1000 took 7.309s\n",
            "The Discriminator loss is 2.6622650623321533 and the Generator loss is 3.7111716270446777.\n",
            "Epoch 480 of 1000 took 7.296s\n",
            "The Discriminator loss is 1.4163352251052856 and the Generator loss is 1.4763659238815308.\n",
            "Epoch 481 of 1000 took 7.301s\n",
            "The Discriminator loss is 2.1949825286865234 and the Generator loss is 2.8675377368927.\n",
            "Epoch 482 of 1000 took 7.232s\n",
            "The Discriminator loss is 2.1683077812194824 and the Generator loss is 1.3398977518081665.\n",
            "Epoch 483 of 1000 took 7.227s\n",
            "The Discriminator loss is 1.207122802734375 and the Generator loss is 1.1153663396835327.\n",
            "Epoch 484 of 1000 took 7.215s\n",
            "The Discriminator loss is 1.1828542947769165 and the Generator loss is 1.9134739637374878.\n",
            "Epoch 485 of 1000 took 7.198s\n",
            "The Discriminator loss is 2.20552921295166 and the Generator loss is 0.8875865340232849.\n",
            "Epoch 486 of 1000 took 7.177s\n",
            "The Discriminator loss is 2.740471363067627 and the Generator loss is 0.7028824090957642.\n",
            "Epoch 487 of 1000 took 7.268s\n",
            "The Discriminator loss is 2.4797801971435547 and the Generator loss is 1.8319222927093506.\n",
            "Epoch 488 of 1000 took 7.298s\n",
            "The Discriminator loss is 1.3014500141143799 and the Generator loss is 3.408111572265625.\n",
            "Epoch 489 of 1000 took 7.319s\n",
            "The Discriminator loss is 2.347784996032715 and the Generator loss is 0.8908933401107788.\n",
            "Epoch 490 of 1000 took 7.277s\n",
            "The Discriminator loss is 2.631178855895996 and the Generator loss is 1.0096768140792847.\n",
            "Epoch 491 of 1000 took 7.165s\n",
            "The Discriminator loss is 3.911654233932495 and the Generator loss is 1.077376365661621.\n",
            "Epoch 492 of 1000 took 7.003s\n",
            "The Discriminator loss is 0.8210457563400269 and the Generator loss is 1.530495285987854.\n",
            "Epoch 493 of 1000 took 6.997s\n",
            "The Discriminator loss is 0.883776068687439 and the Generator loss is 5.049755096435547.\n",
            "Epoch 494 of 1000 took 6.971s\n",
            "The Discriminator loss is 1.3251404762268066 and the Generator loss is 1.1515660285949707.\n",
            "Epoch 495 of 1000 took 7.035s\n",
            "The Discriminator loss is 1.8580933809280396 and the Generator loss is 0.8087604641914368.\n",
            "Epoch 496 of 1000 took 6.997s\n",
            "The Discriminator loss is 3.4830875396728516 and the Generator loss is 0.6754363179206848.\n",
            "Epoch 497 of 1000 took 6.977s\n",
            "The Discriminator loss is 1.5571807622909546 and the Generator loss is 1.5593470335006714.\n",
            "Epoch 498 of 1000 took 7.142s\n",
            "The Discriminator loss is 1.183075189590454 and the Generator loss is 1.6048614978790283.\n",
            "Epoch 499 of 1000 took 7.524s\n",
            "The Discriminator loss is 0.6947630047798157 and the Generator loss is 7.959188461303711.\n",
            "Epoch 500 of 1000 took 7.297s\n",
            "The Discriminator loss is 2.52459979057312 and the Generator loss is 0.5573806762695312.\n",
            "Epoch 501 of 1000 took 7.143s\n",
            "The Discriminator loss is 1.784841537475586 and the Generator loss is 0.9897141456604004.\n",
            "Now Images are being generated\n",
            "Epoch 502 of 1000 took 7.097s\n",
            "The Discriminator loss is 2.360290050506592 and the Generator loss is 2.1750588417053223.\n",
            "Epoch 503 of 1000 took 7.111s\n",
            "The Discriminator loss is 2.3745598793029785 and the Generator loss is 1.356278896331787.\n",
            "Epoch 504 of 1000 took 7.015s\n",
            "The Discriminator loss is 3.8738226890563965 and the Generator loss is 1.0441136360168457.\n",
            "Epoch 505 of 1000 took 7.052s\n",
            "The Discriminator loss is 1.7888071537017822 and the Generator loss is 1.113063097000122.\n",
            "Epoch 506 of 1000 took 7.041s\n",
            "The Discriminator loss is 2.1854000091552734 and the Generator loss is 1.5848114490509033.\n",
            "Epoch 507 of 1000 took 6.817s\n",
            "The Discriminator loss is 1.5653858184814453 and the Generator loss is 2.442850112915039.\n",
            "Epoch 508 of 1000 took 6.921s\n",
            "The Discriminator loss is 2.4238154888153076 and the Generator loss is 1.1003966331481934.\n",
            "Epoch 509 of 1000 took 6.880s\n",
            "The Discriminator loss is 2.718078851699829 and the Generator loss is 1.1498383283615112.\n",
            "Epoch 510 of 1000 took 6.901s\n",
            "The Discriminator loss is 1.8355350494384766 and the Generator loss is 1.51381516456604.\n",
            "Epoch 511 of 1000 took 6.987s\n",
            "The Discriminator loss is 2.938906192779541 and the Generator loss is 0.8680273294448853.\n",
            "Epoch 512 of 1000 took 7.016s\n",
            "The Discriminator loss is 1.7484052181243896 and the Generator loss is 0.9861758351325989.\n",
            "Epoch 513 of 1000 took 7.101s\n",
            "The Discriminator loss is 1.6995487213134766 and the Generator loss is 1.618238091468811.\n",
            "Epoch 514 of 1000 took 7.201s\n",
            "The Discriminator loss is 5.100261688232422 and the Generator loss is 0.7153480052947998.\n",
            "Epoch 515 of 1000 took 7.202s\n",
            "The Discriminator loss is 1.9450592994689941 and the Generator loss is 1.6208304166793823.\n",
            "Epoch 516 of 1000 took 7.172s\n",
            "The Discriminator loss is 1.5863935947418213 and the Generator loss is 0.8610853552818298.\n",
            "Epoch 517 of 1000 took 6.846s\n",
            "The Discriminator loss is 1.4120076894760132 and the Generator loss is 3.8667659759521484.\n",
            "Epoch 518 of 1000 took 6.993s\n",
            "The Discriminator loss is 1.5627636909484863 and the Generator loss is 1.6475787162780762.\n",
            "Epoch 519 of 1000 took 6.942s\n",
            "The Discriminator loss is 1.885796070098877 and the Generator loss is 1.4063551425933838.\n",
            "Epoch 520 of 1000 took 6.769s\n",
            "The Discriminator loss is 1.4087905883789062 and the Generator loss is 1.1855391263961792.\n",
            "Epoch 521 of 1000 took 6.874s\n",
            "The Discriminator loss is 3.822286605834961 and the Generator loss is 1.7264882326126099.\n",
            "Epoch 522 of 1000 took 6.850s\n",
            "The Discriminator loss is 3.1972157955169678 and the Generator loss is 4.051873683929443.\n",
            "Epoch 523 of 1000 took 6.852s\n",
            "The Discriminator loss is 1.120524525642395 and the Generator loss is 2.2531278133392334.\n",
            "Epoch 524 of 1000 took 6.802s\n",
            "The Discriminator loss is 2.06504487991333 and the Generator loss is 1.1372243165969849.\n",
            "Epoch 525 of 1000 took 6.770s\n",
            "The Discriminator loss is 1.7308628559112549 and the Generator loss is 1.3363182544708252.\n",
            "Epoch 526 of 1000 took 6.733s\n",
            "The Discriminator loss is 2.9010751247406006 and the Generator loss is 1.5486317873001099.\n",
            "Epoch 527 of 1000 took 6.712s\n",
            "The Discriminator loss is 1.6037378311157227 and the Generator loss is 0.9439142346382141.\n",
            "Epoch 528 of 1000 took 6.713s\n",
            "The Discriminator loss is 1.4246273040771484 and the Generator loss is 1.6827788352966309.\n",
            "Epoch 529 of 1000 took 6.780s\n",
            "The Discriminator loss is 2.502962589263916 and the Generator loss is 0.7380314469337463.\n",
            "Epoch 530 of 1000 took 6.758s\n",
            "The Discriminator loss is 1.2351346015930176 and the Generator loss is 7.054055213928223.\n",
            "Epoch 531 of 1000 took 6.909s\n",
            "The Discriminator loss is 2.9009175300598145 and the Generator loss is 0.40473702549934387.\n",
            "Epoch 532 of 1000 took 6.818s\n",
            "The Discriminator loss is 1.2297956943511963 and the Generator loss is 2.274167060852051.\n",
            "Epoch 533 of 1000 took 6.777s\n",
            "The Discriminator loss is 1.4661660194396973 and the Generator loss is 0.9044684767723083.\n",
            "Epoch 534 of 1000 took 6.819s\n",
            "The Discriminator loss is 1.960066318511963 and the Generator loss is 1.0857534408569336.\n",
            "Epoch 535 of 1000 took 6.898s\n",
            "The Discriminator loss is 0.934857189655304 and the Generator loss is 1.4947110414505005.\n",
            "Epoch 536 of 1000 took 7.026s\n",
            "The Discriminator loss is 1.9008190631866455 and the Generator loss is 1.5754318237304688.\n",
            "Epoch 537 of 1000 took 7.081s\n",
            "The Discriminator loss is 1.648776888847351 and the Generator loss is 1.4698193073272705.\n",
            "Epoch 538 of 1000 took 6.963s\n",
            "The Discriminator loss is 1.4652997255325317 and the Generator loss is 1.461601972579956.\n",
            "Epoch 539 of 1000 took 7.038s\n",
            "The Discriminator loss is 1.8513391017913818 and the Generator loss is 1.4855024814605713.\n",
            "Epoch 540 of 1000 took 7.104s\n",
            "The Discriminator loss is 1.5494153499603271 and the Generator loss is 4.323977947235107.\n",
            "Epoch 541 of 1000 took 7.142s\n",
            "The Discriminator loss is 2.900286912918091 and the Generator loss is 0.7380104660987854.\n",
            "Epoch 542 of 1000 took 7.012s\n",
            "The Discriminator loss is 1.6596416234970093 and the Generator loss is 1.0618886947631836.\n",
            "Epoch 543 of 1000 took 7.241s\n",
            "The Discriminator loss is 1.7066919803619385 and the Generator loss is 1.506517767906189.\n",
            "Epoch 544 of 1000 took 7.560s\n",
            "The Discriminator loss is 2.319127082824707 and the Generator loss is 0.8532109260559082.\n",
            "Epoch 545 of 1000 took 7.123s\n",
            "The Discriminator loss is 2.0611701011657715 and the Generator loss is 0.6865009069442749.\n",
            "Epoch 546 of 1000 took 7.076s\n",
            "The Discriminator loss is 0.892501950263977 and the Generator loss is 1.562718391418457.\n",
            "Epoch 547 of 1000 took 7.054s\n",
            "The Discriminator loss is 1.6586403846740723 and the Generator loss is 0.7340509295463562.\n",
            "Epoch 548 of 1000 took 7.173s\n",
            "The Discriminator loss is 2.1592962741851807 and the Generator loss is 2.3210649490356445.\n",
            "Epoch 549 of 1000 took 7.063s\n",
            "The Discriminator loss is 1.2001798152923584 and the Generator loss is 5.361056327819824.\n",
            "Epoch 550 of 1000 took 7.015s\n",
            "The Discriminator loss is 1.9740190505981445 and the Generator loss is 1.1419488191604614.\n",
            "Epoch 551 of 1000 took 7.001s\n",
            "The Discriminator loss is 1.934770107269287 and the Generator loss is 1.077519178390503.\n",
            "Epoch 552 of 1000 took 6.980s\n",
            "The Discriminator loss is 0.694238007068634 and the Generator loss is 6.860157012939453.\n",
            "Epoch 553 of 1000 took 7.027s\n",
            "The Discriminator loss is 1.843526840209961 and the Generator loss is 4.067659854888916.\n",
            "Epoch 554 of 1000 took 7.010s\n",
            "The Discriminator loss is 1.3067694902420044 and the Generator loss is 1.5864259004592896.\n",
            "Epoch 555 of 1000 took 7.116s\n",
            "The Discriminator loss is 0.9256924390792847 and the Generator loss is 2.0274479389190674.\n",
            "Epoch 556 of 1000 took 7.180s\n",
            "The Discriminator loss is 1.632503867149353 and the Generator loss is 1.3239701986312866.\n",
            "Epoch 557 of 1000 took 7.199s\n",
            "The Discriminator loss is 1.2696603536605835 and the Generator loss is 1.1653504371643066.\n",
            "Epoch 558 of 1000 took 7.185s\n",
            "The Discriminator loss is 1.757279872894287 and the Generator loss is 0.957212507724762.\n",
            "Epoch 559 of 1000 took 7.240s\n",
            "The Discriminator loss is 1.4205811023712158 and the Generator loss is 2.342886447906494.\n",
            "Epoch 560 of 1000 took 7.122s\n",
            "The Discriminator loss is 1.762976884841919 and the Generator loss is 4.201406955718994.\n",
            "Epoch 561 of 1000 took 7.142s\n",
            "The Discriminator loss is 1.0825119018554688 and the Generator loss is 4.699872970581055.\n",
            "Epoch 562 of 1000 took 7.062s\n",
            "The Discriminator loss is 1.3390858173370361 and the Generator loss is 1.3001629114151.\n",
            "Epoch 563 of 1000 took 6.982s\n",
            "The Discriminator loss is 2.62937068939209 and the Generator loss is 0.8806567192077637.\n",
            "Epoch 564 of 1000 took 6.977s\n",
            "The Discriminator loss is 2.1001734733581543 and the Generator loss is 1.4568681716918945.\n",
            "Epoch 565 of 1000 took 7.008s\n",
            "The Discriminator loss is 2.339702844619751 and the Generator loss is 0.8062732815742493.\n",
            "Epoch 566 of 1000 took 6.895s\n",
            "The Discriminator loss is 2.183657169342041 and the Generator loss is 0.889183521270752.\n",
            "Epoch 567 of 1000 took 6.928s\n",
            "The Discriminator loss is 1.5439099073410034 and the Generator loss is 0.9539201259613037.\n",
            "Epoch 568 of 1000 took 7.153s\n",
            "The Discriminator loss is 2.6917572021484375 and the Generator loss is 0.7232928276062012.\n",
            "Epoch 569 of 1000 took 7.222s\n",
            "The Discriminator loss is 2.032320022583008 and the Generator loss is 0.6973231434822083.\n",
            "Epoch 570 of 1000 took 7.293s\n",
            "The Discriminator loss is 2.2531375885009766 and the Generator loss is 0.7140652537345886.\n",
            "Epoch 571 of 1000 took 7.337s\n",
            "The Discriminator loss is 1.3550896644592285 and the Generator loss is 1.7322918176651.\n",
            "Epoch 572 of 1000 took 7.026s\n",
            "The Discriminator loss is 2.190122604370117 and the Generator loss is 1.6650094985961914.\n",
            "Epoch 573 of 1000 took 6.899s\n",
            "The Discriminator loss is 2.629178047180176 and the Generator loss is 0.9438092708587646.\n",
            "Epoch 574 of 1000 took 6.762s\n",
            "The Discriminator loss is 1.9717092514038086 and the Generator loss is 0.7034974694252014.\n",
            "Epoch 575 of 1000 took 6.940s\n",
            "The Discriminator loss is 1.878298044204712 and the Generator loss is 0.9975749850273132.\n",
            "Epoch 576 of 1000 took 6.876s\n",
            "The Discriminator loss is 1.8058245182037354 and the Generator loss is 1.3978291749954224.\n",
            "Epoch 577 of 1000 took 6.869s\n",
            "The Discriminator loss is 1.7259714603424072 and the Generator loss is 1.2700622081756592.\n",
            "Epoch 578 of 1000 took 6.848s\n",
            "The Discriminator loss is 3.7739453315734863 and the Generator loss is 1.1405481100082397.\n",
            "Epoch 579 of 1000 took 6.888s\n",
            "The Discriminator loss is 0.32926255464553833 and the Generator loss is 5.464073657989502.\n",
            "Epoch 580 of 1000 took 6.913s\n",
            "The Discriminator loss is 0.28205040097236633 and the Generator loss is 4.382089138031006.\n",
            "Epoch 581 of 1000 took 6.864s\n",
            "The Discriminator loss is 2.2121336460113525 and the Generator loss is 3.9980509281158447.\n",
            "Epoch 582 of 1000 took 6.783s\n",
            "The Discriminator loss is 2.0339691638946533 and the Generator loss is 1.2501351833343506.\n",
            "Epoch 583 of 1000 took 6.853s\n",
            "The Discriminator loss is 1.656878113746643 and the Generator loss is 1.4003214836120605.\n",
            "Epoch 584 of 1000 took 6.921s\n",
            "The Discriminator loss is 2.213653564453125 and the Generator loss is 1.0459520816802979.\n",
            "Epoch 585 of 1000 took 6.969s\n",
            "The Discriminator loss is 2.727555513381958 and the Generator loss is 0.9562495350837708.\n",
            "Epoch 586 of 1000 took 7.016s\n",
            "The Discriminator loss is 1.150597333908081 and the Generator loss is 3.103994369506836.\n",
            "Epoch 587 of 1000 took 7.272s\n",
            "The Discriminator loss is 1.3557536602020264 and the Generator loss is 0.9982109069824219.\n",
            "Epoch 588 of 1000 took 7.715s\n",
            "The Discriminator loss is 2.519805908203125 and the Generator loss is 1.096206545829773.\n",
            "Epoch 589 of 1000 took 6.975s\n",
            "The Discriminator loss is 2.3967323303222656 and the Generator loss is 2.197056770324707.\n",
            "Epoch 590 of 1000 took 6.956s\n",
            "The Discriminator loss is 2.4882967472076416 and the Generator loss is 3.8056418895721436.\n",
            "Epoch 591 of 1000 took 6.862s\n",
            "The Discriminator loss is 1.8034825325012207 and the Generator loss is 1.6519901752471924.\n",
            "Epoch 592 of 1000 took 7.071s\n",
            "The Discriminator loss is 2.001124858856201 and the Generator loss is 0.8085453510284424.\n",
            "Epoch 593 of 1000 took 7.081s\n",
            "The Discriminator loss is 1.4540345668792725 and the Generator loss is 1.4168381690979004.\n",
            "Epoch 594 of 1000 took 7.066s\n",
            "The Discriminator loss is 1.6237576007843018 and the Generator loss is 2.0631232261657715.\n",
            "Epoch 595 of 1000 took 6.970s\n",
            "The Discriminator loss is 1.6442513465881348 and the Generator loss is 1.4024418592453003.\n",
            "Epoch 596 of 1000 took 7.018s\n",
            "The Discriminator loss is 2.0661051273345947 and the Generator loss is 1.5724374055862427.\n",
            "Epoch 597 of 1000 took 7.112s\n",
            "The Discriminator loss is 2.3386142253875732 and the Generator loss is 1.1375001668930054.\n",
            "Epoch 598 of 1000 took 7.111s\n",
            "The Discriminator loss is 1.9453799724578857 and the Generator loss is 1.1003222465515137.\n",
            "Epoch 599 of 1000 took 7.122s\n",
            "The Discriminator loss is 1.3192787170410156 and the Generator loss is 1.5368255376815796.\n",
            "Epoch 600 of 1000 took 7.176s\n",
            "The Discriminator loss is 2.1026411056518555 and the Generator loss is 0.7696054577827454.\n",
            "Epoch 601 of 1000 took 7.118s\n",
            "The Discriminator loss is 3.007513999938965 and the Generator loss is 0.5574233531951904.\n",
            "Now Images are being generated\n",
            "Epoch 602 of 1000 took 7.255s\n",
            "The Discriminator loss is 1.2875118255615234 and the Generator loss is 1.2019273042678833.\n",
            "Epoch 603 of 1000 took 7.128s\n",
            "The Discriminator loss is 1.1566210985183716 and the Generator loss is 1.261134147644043.\n",
            "Epoch 604 of 1000 took 7.015s\n",
            "The Discriminator loss is 1.6647179126739502 and the Generator loss is 1.0383384227752686.\n",
            "Epoch 605 of 1000 took 7.168s\n",
            "The Discriminator loss is 2.675896167755127 and the Generator loss is 1.7933378219604492.\n",
            "Epoch 606 of 1000 took 7.107s\n",
            "The Discriminator loss is 2.033555507659912 and the Generator loss is 1.8185409307479858.\n",
            "Epoch 607 of 1000 took 7.053s\n",
            "The Discriminator loss is 1.3573287725448608 and the Generator loss is 3.339315414428711.\n",
            "Epoch 608 of 1000 took 7.003s\n",
            "The Discriminator loss is 2.7598485946655273 and the Generator loss is 4.126258850097656.\n",
            "Epoch 609 of 1000 took 7.073s\n",
            "The Discriminator loss is 2.115652561187744 and the Generator loss is 0.8700112700462341.\n",
            "Epoch 610 of 1000 took 6.909s\n",
            "The Discriminator loss is 3.1183714866638184 and the Generator loss is 0.6908498406410217.\n",
            "Epoch 611 of 1000 took 6.832s\n",
            "The Discriminator loss is 0.8319730758666992 and the Generator loss is 2.284787893295288.\n",
            "Epoch 612 of 1000 took 6.837s\n",
            "The Discriminator loss is 2.5012974739074707 and the Generator loss is 1.743983268737793.\n",
            "Epoch 613 of 1000 took 6.810s\n",
            "The Discriminator loss is 1.0070316791534424 and the Generator loss is 1.3683139085769653.\n",
            "Epoch 614 of 1000 took 6.935s\n",
            "The Discriminator loss is 3.5086989402770996 and the Generator loss is 1.1466847658157349.\n",
            "Epoch 615 of 1000 took 7.019s\n",
            "The Discriminator loss is 1.284031629562378 and the Generator loss is 1.2626914978027344.\n",
            "Epoch 616 of 1000 took 6.873s\n",
            "The Discriminator loss is 4.42957878112793 and the Generator loss is 0.7941359877586365.\n",
            "Epoch 617 of 1000 took 6.808s\n",
            "The Discriminator loss is 1.416952133178711 and the Generator loss is 1.1531370878219604.\n",
            "Epoch 618 of 1000 took 6.827s\n",
            "The Discriminator loss is 2.726073980331421 and the Generator loss is 0.7654184699058533.\n",
            "Epoch 619 of 1000 took 6.840s\n",
            "The Discriminator loss is 1.2805136442184448 and the Generator loss is 3.1014342308044434.\n",
            "Epoch 620 of 1000 took 6.833s\n",
            "The Discriminator loss is 1.0553271770477295 and the Generator loss is 3.1672370433807373.\n",
            "Epoch 621 of 1000 took 6.781s\n",
            "The Discriminator loss is 1.0049079656600952 and the Generator loss is 4.937002182006836.\n",
            "Epoch 622 of 1000 took 6.842s\n",
            "The Discriminator loss is 2.1088061332702637 and the Generator loss is 0.9060572981834412.\n",
            "Epoch 623 of 1000 took 6.836s\n",
            "The Discriminator loss is 3.2235608100891113 and the Generator loss is 0.9233050346374512.\n",
            "Epoch 624 of 1000 took 6.730s\n",
            "The Discriminator loss is 2.5332090854644775 and the Generator loss is 2.898672580718994.\n",
            "Epoch 625 of 1000 took 6.845s\n",
            "The Discriminator loss is 0.7393199801445007 and the Generator loss is 6.318923473358154.\n",
            "Epoch 626 of 1000 took 6.920s\n",
            "The Discriminator loss is 0.5246154069900513 and the Generator loss is 7.209803581237793.\n",
            "Epoch 627 of 1000 took 7.095s\n",
            "The Discriminator loss is 2.072396755218506 and the Generator loss is 0.8320678472518921.\n",
            "Epoch 628 of 1000 took 7.113s\n",
            "The Discriminator loss is 2.0142154693603516 and the Generator loss is 0.9404045939445496.\n",
            "Epoch 629 of 1000 took 7.103s\n",
            "The Discriminator loss is 2.450288772583008 and the Generator loss is 1.8669184446334839.\n",
            "Epoch 630 of 1000 took 6.897s\n",
            "The Discriminator loss is 1.954798936843872 and the Generator loss is 0.6096047759056091.\n",
            "Epoch 631 of 1000 took 7.206s\n",
            "The Discriminator loss is 2.172480821609497 and the Generator loss is 2.1527392864227295.\n",
            "Epoch 632 of 1000 took 7.700s\n",
            "The Discriminator loss is 1.0435495376586914 and the Generator loss is 3.8446426391601562.\n",
            "Epoch 633 of 1000 took 7.116s\n",
            "The Discriminator loss is 2.727534294128418 and the Generator loss is 2.9674904346466064.\n",
            "Epoch 634 of 1000 took 6.897s\n",
            "The Discriminator loss is 0.8855205774307251 and the Generator loss is 2.657045364379883.\n",
            "Epoch 635 of 1000 took 6.867s\n",
            "The Discriminator loss is 1.445082187652588 and the Generator loss is 1.0036978721618652.\n",
            "Epoch 636 of 1000 took 7.149s\n",
            "The Discriminator loss is 1.5625355243682861 and the Generator loss is 3.6552486419677734.\n",
            "Epoch 637 of 1000 took 7.012s\n",
            "The Discriminator loss is 0.3409188389778137 and the Generator loss is 5.7156147956848145.\n",
            "Epoch 638 of 1000 took 6.935s\n",
            "The Discriminator loss is 1.5189180374145508 and the Generator loss is 2.1643717288970947.\n",
            "Epoch 639 of 1000 took 6.930s\n",
            "The Discriminator loss is 1.7996312379837036 and the Generator loss is 1.719536542892456.\n",
            "Epoch 640 of 1000 took 7.097s\n",
            "The Discriminator loss is 1.0474145412445068 and the Generator loss is 3.3723154067993164.\n",
            "Epoch 641 of 1000 took 7.141s\n",
            "The Discriminator loss is 1.3575730323791504 and the Generator loss is 1.4234488010406494.\n",
            "Epoch 642 of 1000 took 7.216s\n",
            "The Discriminator loss is 2.6006722450256348 and the Generator loss is 1.8011013269424438.\n",
            "Epoch 643 of 1000 took 7.254s\n",
            "The Discriminator loss is 1.42270827293396 and the Generator loss is 0.8929667472839355.\n",
            "Epoch 644 of 1000 took 7.332s\n",
            "The Discriminator loss is 1.5464301109313965 and the Generator loss is 1.09250807762146.\n",
            "Epoch 645 of 1000 took 7.391s\n",
            "The Discriminator loss is 1.2435576915740967 and the Generator loss is 4.233077526092529.\n",
            "Epoch 646 of 1000 took 7.374s\n",
            "The Discriminator loss is 2.178138017654419 and the Generator loss is 1.1728671789169312.\n",
            "Epoch 647 of 1000 took 7.323s\n",
            "The Discriminator loss is 2.5180373191833496 and the Generator loss is 0.8157212734222412.\n",
            "Epoch 648 of 1000 took 7.240s\n",
            "The Discriminator loss is 2.0269200801849365 and the Generator loss is 1.6341341733932495.\n",
            "Epoch 649 of 1000 took 7.234s\n",
            "The Discriminator loss is 1.8905662298202515 and the Generator loss is 0.6841888427734375.\n",
            "Epoch 650 of 1000 took 7.279s\n",
            "The Discriminator loss is 1.6798570156097412 and the Generator loss is 1.0570769309997559.\n",
            "Epoch 651 of 1000 took 7.345s\n",
            "The Discriminator loss is 2.664194345474243 and the Generator loss is 0.5026876926422119.\n",
            "Epoch 652 of 1000 took 7.302s\n",
            "The Discriminator loss is 1.7635548114776611 and the Generator loss is 1.0584852695465088.\n",
            "Epoch 653 of 1000 took 7.253s\n",
            "The Discriminator loss is 2.26120924949646 and the Generator loss is 0.9007795453071594.\n",
            "Epoch 654 of 1000 took 7.290s\n",
            "The Discriminator loss is 1.8791258335113525 and the Generator loss is 8.89760684967041.\n",
            "Epoch 655 of 1000 took 7.309s\n",
            "The Discriminator loss is 2.1508493423461914 and the Generator loss is 2.3063182830810547.\n",
            "Epoch 656 of 1000 took 7.301s\n",
            "The Discriminator loss is 2.418156623840332 and the Generator loss is 0.6924460530281067.\n",
            "Epoch 657 of 1000 took 7.247s\n",
            "The Discriminator loss is 1.728567361831665 and the Generator loss is 1.5360305309295654.\n",
            "Epoch 658 of 1000 took 7.110s\n",
            "The Discriminator loss is 1.9870128631591797 and the Generator loss is 1.2498507499694824.\n",
            "Epoch 659 of 1000 took 6.833s\n",
            "The Discriminator loss is 2.0699501037597656 and the Generator loss is 1.3913546800613403.\n",
            "Epoch 660 of 1000 took 6.808s\n",
            "The Discriminator loss is 1.7560210227966309 and the Generator loss is 0.9127549529075623.\n",
            "Epoch 661 of 1000 took 7.037s\n",
            "The Discriminator loss is 4.264739990234375 and the Generator loss is 0.8557306528091431.\n",
            "Epoch 662 of 1000 took 7.007s\n",
            "The Discriminator loss is 1.8480379581451416 and the Generator loss is 1.0679560899734497.\n",
            "Epoch 663 of 1000 took 6.853s\n",
            "The Discriminator loss is 2.818434238433838 and the Generator loss is 0.9768022894859314.\n",
            "Epoch 664 of 1000 took 6.871s\n",
            "The Discriminator loss is 1.4371100664138794 and the Generator loss is 2.1393752098083496.\n",
            "Epoch 665 of 1000 took 6.942s\n",
            "The Discriminator loss is 1.7210237979888916 and the Generator loss is 1.8446669578552246.\n",
            "Epoch 666 of 1000 took 7.023s\n",
            "The Discriminator loss is 2.781911611557007 and the Generator loss is 0.6324498653411865.\n",
            "Epoch 667 of 1000 took 7.222s\n",
            "The Discriminator loss is 1.482147216796875 and the Generator loss is 1.0544018745422363.\n",
            "Epoch 668 of 1000 took 6.925s\n",
            "The Discriminator loss is 2.513383150100708 and the Generator loss is 0.5601045489311218.\n",
            "Epoch 669 of 1000 took 6.760s\n",
            "The Discriminator loss is 1.6287105083465576 and the Generator loss is 6.0488972663879395.\n",
            "Epoch 670 of 1000 took 6.818s\n",
            "The Discriminator loss is 2.1996307373046875 and the Generator loss is 1.2994457483291626.\n",
            "Epoch 671 of 1000 took 6.816s\n",
            "The Discriminator loss is 2.127115249633789 and the Generator loss is 1.237154483795166.\n",
            "Epoch 672 of 1000 took 6.878s\n",
            "The Discriminator loss is 1.9982514381408691 and the Generator loss is 3.414534330368042.\n",
            "Epoch 673 of 1000 took 6.960s\n",
            "The Discriminator loss is 2.4894180297851562 and the Generator loss is 2.3471970558166504.\n",
            "Epoch 674 of 1000 took 6.881s\n",
            "The Discriminator loss is 1.9959115982055664 and the Generator loss is 1.1406924724578857.\n",
            "Epoch 675 of 1000 took 7.222s\n",
            "The Discriminator loss is 1.4357786178588867 and the Generator loss is 3.9234752655029297.\n",
            "Epoch 676 of 1000 took 7.299s\n",
            "The Discriminator loss is 1.3752490282058716 and the Generator loss is 1.1817055940628052.\n",
            "Epoch 677 of 1000 took 7.022s\n",
            "The Discriminator loss is 3.2046940326690674 and the Generator loss is 1.5704156160354614.\n",
            "Epoch 678 of 1000 took 6.819s\n",
            "The Discriminator loss is 2.0412893295288086 and the Generator loss is 0.8933947086334229.\n",
            "Epoch 679 of 1000 took 6.919s\n",
            "The Discriminator loss is 1.3875982761383057 and the Generator loss is 1.7545063495635986.\n",
            "Epoch 680 of 1000 took 6.868s\n",
            "The Discriminator loss is 1.595306634902954 and the Generator loss is 0.8079755902290344.\n",
            "Epoch 681 of 1000 took 7.035s\n",
            "The Discriminator loss is 1.735887885093689 and the Generator loss is 0.9041538834571838.\n",
            "Epoch 682 of 1000 took 7.064s\n",
            "The Discriminator loss is 2.166175365447998 and the Generator loss is 0.6772503852844238.\n",
            "Epoch 683 of 1000 took 7.164s\n",
            "The Discriminator loss is 1.482015609741211 and the Generator loss is 6.4395751953125.\n",
            "Epoch 684 of 1000 took 6.894s\n",
            "The Discriminator loss is 0.9117248058319092 and the Generator loss is 2.044830799102783.\n",
            "Epoch 685 of 1000 took 6.910s\n",
            "The Discriminator loss is 1.5342737436294556 and the Generator loss is 1.3211368322372437.\n",
            "Epoch 686 of 1000 took 6.857s\n",
            "The Discriminator loss is 2.3290786743164062 and the Generator loss is 3.0159168243408203.\n",
            "Epoch 687 of 1000 took 7.026s\n",
            "The Discriminator loss is 2.464754343032837 and the Generator loss is 1.0218831300735474.\n",
            "Epoch 688 of 1000 took 6.996s\n",
            "The Discriminator loss is 1.5522124767303467 and the Generator loss is 1.0216069221496582.\n",
            "Epoch 689 of 1000 took 7.014s\n",
            "The Discriminator loss is 0.3660147786140442 and the Generator loss is 5.130242824554443.\n",
            "Epoch 690 of 1000 took 6.933s\n",
            "The Discriminator loss is 0.22437399625778198 and the Generator loss is 6.541608810424805.\n",
            "Epoch 691 of 1000 took 7.087s\n",
            "The Discriminator loss is 2.9138479232788086 and the Generator loss is 1.4850213527679443.\n",
            "Epoch 692 of 1000 took 7.088s\n",
            "The Discriminator loss is 1.4112730026245117 and the Generator loss is 2.575862169265747.\n",
            "Epoch 693 of 1000 took 6.949s\n",
            "The Discriminator loss is 2.6207146644592285 and the Generator loss is 1.863640546798706.\n",
            "Epoch 694 of 1000 took 7.084s\n",
            "The Discriminator loss is 2.223717212677002 and the Generator loss is 0.6796082854270935.\n",
            "Epoch 695 of 1000 took 6.898s\n",
            "The Discriminator loss is 1.3253836631774902 and the Generator loss is 1.0606906414031982.\n",
            "Epoch 696 of 1000 took 6.874s\n",
            "The Discriminator loss is 3.043300151824951 and the Generator loss is 1.9006140232086182.\n",
            "Epoch 697 of 1000 took 6.853s\n",
            "The Discriminator loss is 1.3144505023956299 and the Generator loss is 1.787158489227295.\n",
            "Epoch 698 of 1000 took 6.768s\n",
            "The Discriminator loss is 0.4093915820121765 and the Generator loss is 6.587561130523682.\n",
            "Epoch 699 of 1000 took 7.028s\n",
            "The Discriminator loss is 0.5873162150382996 and the Generator loss is 3.7443108558654785.\n",
            "Epoch 700 of 1000 took 7.155s\n",
            "The Discriminator loss is 1.5806355476379395 and the Generator loss is 2.3173701763153076.\n",
            "Epoch 701 of 1000 took 7.166s\n",
            "The Discriminator loss is 1.3853745460510254 and the Generator loss is 0.8715707659721375.\n",
            "Now Images are being generated\n",
            "Epoch 702 of 1000 took 7.119s\n",
            "The Discriminator loss is 2.6587703227996826 and the Generator loss is 0.6029359698295593.\n",
            "Epoch 703 of 1000 took 7.227s\n",
            "The Discriminator loss is 1.0650981664657593 and the Generator loss is 1.585248589515686.\n",
            "Epoch 704 of 1000 took 7.284s\n",
            "The Discriminator loss is 4.4307861328125 and the Generator loss is 0.5576174259185791.\n",
            "Epoch 705 of 1000 took 7.318s\n",
            "The Discriminator loss is 3.301158905029297 and the Generator loss is 3.3747479915618896.\n",
            "Epoch 706 of 1000 took 7.271s\n",
            "The Discriminator loss is 2.567768096923828 and the Generator loss is 1.6307597160339355.\n",
            "Epoch 707 of 1000 took 7.201s\n",
            "The Discriminator loss is 1.5369545221328735 and the Generator loss is 1.7866389751434326.\n",
            "Epoch 708 of 1000 took 7.190s\n",
            "The Discriminator loss is 1.8793829679489136 and the Generator loss is 0.9886030554771423.\n",
            "Epoch 709 of 1000 took 7.157s\n",
            "The Discriminator loss is 1.5478670597076416 and the Generator loss is 0.9845645427703857.\n",
            "Epoch 710 of 1000 took 7.210s\n",
            "The Discriminator loss is 2.829346179962158 and the Generator loss is 1.3460158109664917.\n",
            "Epoch 711 of 1000 took 7.352s\n",
            "The Discriminator loss is 1.7038527727127075 and the Generator loss is 0.8736613392829895.\n",
            "Epoch 712 of 1000 took 7.383s\n",
            "The Discriminator loss is 2.870915412902832 and the Generator loss is 0.8006163835525513.\n",
            "Epoch 713 of 1000 took 7.213s\n",
            "The Discriminator loss is 4.697279930114746 and the Generator loss is 0.790883481502533.\n",
            "Epoch 714 of 1000 took 7.100s\n",
            "The Discriminator loss is 3.3311715126037598 and the Generator loss is 0.7641502618789673.\n",
            "Epoch 715 of 1000 took 7.132s\n",
            "The Discriminator loss is 1.7166637182235718 and the Generator loss is 4.686751365661621.\n",
            "Epoch 716 of 1000 took 7.084s\n",
            "The Discriminator loss is 1.4258296489715576 and the Generator loss is 2.4252073764801025.\n",
            "Epoch 717 of 1000 took 7.082s\n",
            "The Discriminator loss is 1.4586944580078125 and the Generator loss is 1.0973320007324219.\n",
            "Epoch 718 of 1000 took 7.186s\n",
            "The Discriminator loss is 1.8352251052856445 and the Generator loss is 0.941423237323761.\n",
            "Epoch 719 of 1000 took 7.470s\n",
            "The Discriminator loss is 3.0459933280944824 and the Generator loss is 0.9866412878036499.\n",
            "Epoch 720 of 1000 took 7.497s\n",
            "The Discriminator loss is 4.446231842041016 and the Generator loss is 0.8639720678329468.\n",
            "Epoch 721 of 1000 took 7.172s\n",
            "The Discriminator loss is 1.9083917140960693 and the Generator loss is 1.1632094383239746.\n",
            "Epoch 722 of 1000 took 7.048s\n",
            "The Discriminator loss is 2.4687395095825195 and the Generator loss is 1.1658594608306885.\n",
            "Epoch 723 of 1000 took 7.109s\n",
            "The Discriminator loss is 2.792222023010254 and the Generator loss is 1.1336535215377808.\n",
            "Epoch 724 of 1000 took 7.081s\n",
            "The Discriminator loss is 2.8771166801452637 and the Generator loss is 0.9025739431381226.\n",
            "Epoch 725 of 1000 took 7.129s\n",
            "The Discriminator loss is 1.8616325855255127 and the Generator loss is 0.8179860711097717.\n",
            "Epoch 726 of 1000 took 7.114s\n",
            "The Discriminator loss is 2.1686182022094727 and the Generator loss is 1.3991175889968872.\n",
            "Epoch 727 of 1000 took 6.983s\n",
            "The Discriminator loss is 2.027015447616577 and the Generator loss is 1.1418733596801758.\n",
            "Epoch 728 of 1000 took 6.880s\n",
            "The Discriminator loss is 2.574484348297119 and the Generator loss is 0.859358012676239.\n",
            "Epoch 729 of 1000 took 6.990s\n",
            "The Discriminator loss is 1.768158197402954 and the Generator loss is 0.9213866591453552.\n",
            "Epoch 730 of 1000 took 6.753s\n",
            "The Discriminator loss is 1.7826743125915527 and the Generator loss is 0.8572597503662109.\n",
            "Epoch 731 of 1000 took 6.886s\n",
            "The Discriminator loss is 1.9353853464126587 and the Generator loss is 1.156512975692749.\n",
            "Epoch 732 of 1000 took 6.943s\n",
            "The Discriminator loss is 1.5394628047943115 and the Generator loss is 2.0599193572998047.\n",
            "Epoch 733 of 1000 took 6.687s\n",
            "The Discriminator loss is 1.4510862827301025 and the Generator loss is 1.6389425992965698.\n",
            "Epoch 734 of 1000 took 6.695s\n",
            "The Discriminator loss is 1.8030139207839966 and the Generator loss is 0.8576561808586121.\n",
            "Epoch 735 of 1000 took 6.863s\n",
            "The Discriminator loss is 1.8377344608306885 and the Generator loss is 2.5632379055023193.\n",
            "Epoch 736 of 1000 took 7.055s\n",
            "The Discriminator loss is 2.5497376918792725 and the Generator loss is 1.9782419204711914.\n",
            "Epoch 737 of 1000 took 7.172s\n",
            "The Discriminator loss is 2.3630154132843018 and the Generator loss is 0.6931083798408508.\n",
            "Epoch 738 of 1000 took 7.161s\n",
            "The Discriminator loss is 1.5604133605957031 and the Generator loss is 0.865314245223999.\n",
            "Epoch 739 of 1000 took 7.237s\n",
            "The Discriminator loss is 2.0806469917297363 and the Generator loss is 0.6014918088912964.\n",
            "Epoch 740 of 1000 took 7.198s\n",
            "The Discriminator loss is 0.930672287940979 and the Generator loss is 2.868859052658081.\n",
            "Epoch 741 of 1000 took 7.312s\n",
            "The Discriminator loss is 1.4536011219024658 and the Generator loss is 1.6071940660476685.\n",
            "Epoch 742 of 1000 took 7.348s\n",
            "The Discriminator loss is 4.348319053649902 and the Generator loss is 0.744132936000824.\n",
            "Epoch 743 of 1000 took 7.365s\n",
            "The Discriminator loss is 1.4087520837783813 and the Generator loss is 1.6080375909805298.\n",
            "Epoch 744 of 1000 took 7.352s\n",
            "The Discriminator loss is 2.4810009002685547 and the Generator loss is 0.5119470357894897.\n",
            "Epoch 745 of 1000 took 7.409s\n",
            "The Discriminator loss is 1.6727449893951416 and the Generator loss is 0.7648642659187317.\n",
            "Epoch 746 of 1000 took 7.189s\n",
            "The Discriminator loss is 2.067755937576294 and the Generator loss is 1.6228502988815308.\n",
            "Epoch 747 of 1000 took 6.947s\n",
            "The Discriminator loss is 1.9360657930374146 and the Generator loss is 1.0286974906921387.\n",
            "Epoch 748 of 1000 took 7.059s\n",
            "The Discriminator loss is 1.75913405418396 and the Generator loss is 0.869315505027771.\n",
            "Epoch 749 of 1000 took 7.111s\n",
            "The Discriminator loss is 3.6853344440460205 and the Generator loss is 0.8103376626968384.\n",
            "Epoch 750 of 1000 took 7.048s\n",
            "The Discriminator loss is 1.9523005485534668 and the Generator loss is 0.662306010723114.\n",
            "Epoch 751 of 1000 took 7.077s\n",
            "The Discriminator loss is 3.7454633712768555 and the Generator loss is 1.989546537399292.\n",
            "Epoch 752 of 1000 took 7.027s\n",
            "The Discriminator loss is 2.7047226428985596 and the Generator loss is 0.8560382723808289.\n",
            "Epoch 753 of 1000 took 6.938s\n",
            "The Discriminator loss is 2.3036859035491943 and the Generator loss is 1.3652032613754272.\n",
            "Epoch 754 of 1000 took 6.930s\n",
            "The Discriminator loss is 2.6385068893432617 and the Generator loss is 0.6068857312202454.\n",
            "Epoch 755 of 1000 took 6.922s\n",
            "The Discriminator loss is 1.8113508224487305 and the Generator loss is 1.8853936195373535.\n",
            "Epoch 756 of 1000 took 6.865s\n",
            "The Discriminator loss is 2.0903420448303223 and the Generator loss is 0.8895768523216248.\n",
            "Epoch 757 of 1000 took 6.865s\n",
            "The Discriminator loss is 1.8100552558898926 and the Generator loss is 1.5199660062789917.\n",
            "Epoch 758 of 1000 took 6.890s\n",
            "The Discriminator loss is 2.9397671222686768 and the Generator loss is 1.2310422658920288.\n",
            "Epoch 759 of 1000 took 7.001s\n",
            "The Discriminator loss is 3.405830144882202 and the Generator loss is 0.6456108689308167.\n",
            "Epoch 760 of 1000 took 7.095s\n",
            "The Discriminator loss is 2.413512706756592 and the Generator loss is 0.5595383644104004.\n",
            "Epoch 761 of 1000 took 7.063s\n",
            "The Discriminator loss is 1.9635106325149536 and the Generator loss is 0.815209150314331.\n",
            "Epoch 762 of 1000 took 7.173s\n",
            "The Discriminator loss is 3.574965000152588 and the Generator loss is 0.9559817314147949.\n",
            "Epoch 763 of 1000 took 6.918s\n",
            "The Discriminator loss is 0.7081800103187561 and the Generator loss is 2.709453821182251.\n",
            "Epoch 764 of 1000 took 7.090s\n",
            "The Discriminator loss is 2.4834413528442383 and the Generator loss is 0.8248044848442078.\n",
            "Epoch 765 of 1000 took 6.899s\n",
            "The Discriminator loss is 3.125711441040039 and the Generator loss is 1.8442736864089966.\n",
            "Epoch 766 of 1000 took 6.817s\n",
            "The Discriminator loss is 5.495309829711914 and the Generator loss is 1.0796771049499512.\n",
            "Epoch 767 of 1000 took 6.859s\n",
            "The Discriminator loss is 1.661973237991333 and the Generator loss is 2.9040839672088623.\n",
            "Epoch 768 of 1000 took 7.090s\n",
            "The Discriminator loss is 2.7950992584228516 and the Generator loss is 0.47899386286735535.\n",
            "Epoch 769 of 1000 took 6.778s\n",
            "The Discriminator loss is 2.0302069187164307 and the Generator loss is 1.249133586883545.\n",
            "Epoch 770 of 1000 took 6.712s\n",
            "The Discriminator loss is 1.1117641925811768 and the Generator loss is 1.6479568481445312.\n",
            "Epoch 771 of 1000 took 6.700s\n",
            "The Discriminator loss is 1.6938644647598267 and the Generator loss is 1.2862452268600464.\n",
            "Epoch 772 of 1000 took 6.736s\n",
            "The Discriminator loss is 5.75974178314209 and the Generator loss is 0.45037463307380676.\n",
            "Epoch 773 of 1000 took 6.761s\n",
            "The Discriminator loss is 1.3873481750488281 and the Generator loss is 1.6992652416229248.\n",
            "Epoch 774 of 1000 took 6.875s\n",
            "The Discriminator loss is 1.833204984664917 and the Generator loss is 0.8885596990585327.\n",
            "Epoch 775 of 1000 took 7.015s\n",
            "The Discriminator loss is 1.4090251922607422 and the Generator loss is 1.4093718528747559.\n",
            "Epoch 776 of 1000 took 7.008s\n",
            "The Discriminator loss is 2.4581375122070312 and the Generator loss is 5.797439098358154.\n",
            "Epoch 777 of 1000 took 7.043s\n",
            "The Discriminator loss is 2.3531999588012695 and the Generator loss is 4.40009069442749.\n",
            "Epoch 778 of 1000 took 6.640s\n",
            "The Discriminator loss is 1.7925548553466797 and the Generator loss is 1.314843773841858.\n",
            "Epoch 779 of 1000 took 6.682s\n",
            "The Discriminator loss is 1.6948041915893555 and the Generator loss is 1.3265010118484497.\n",
            "Epoch 780 of 1000 took 6.876s\n",
            "The Discriminator loss is 1.8371951580047607 and the Generator loss is 0.7795889973640442.\n",
            "Epoch 781 of 1000 took 6.812s\n",
            "The Discriminator loss is 1.4585044384002686 and the Generator loss is 1.0742771625518799.\n",
            "Epoch 782 of 1000 took 6.626s\n",
            "The Discriminator loss is 3.3767282962799072 and the Generator loss is 1.1108900308609009.\n",
            "Epoch 783 of 1000 took 6.680s\n",
            "The Discriminator loss is 1.3372153043746948 and the Generator loss is 1.3438994884490967.\n",
            "Epoch 784 of 1000 took 6.792s\n",
            "The Discriminator loss is 1.8471347093582153 and the Generator loss is 1.1761772632598877.\n",
            "Epoch 785 of 1000 took 6.793s\n",
            "The Discriminator loss is 1.7155975103378296 and the Generator loss is 1.0077886581420898.\n",
            "Epoch 786 of 1000 took 6.933s\n",
            "The Discriminator loss is 0.35344740748405457 and the Generator loss is 6.01862907409668.\n",
            "Epoch 787 of 1000 took 6.778s\n",
            "The Discriminator loss is 2.36091947555542 and the Generator loss is 1.6165553331375122.\n",
            "Epoch 788 of 1000 took 6.878s\n",
            "The Discriminator loss is 2.67795991897583 and the Generator loss is 0.9171105027198792.\n",
            "Epoch 789 of 1000 took 6.961s\n",
            "The Discriminator loss is 2.3884599208831787 and the Generator loss is 1.6413007974624634.\n",
            "Epoch 790 of 1000 took 6.873s\n",
            "The Discriminator loss is 1.6399893760681152 and the Generator loss is 1.1355855464935303.\n",
            "Epoch 791 of 1000 took 6.801s\n",
            "The Discriminator loss is 3.0298280715942383 and the Generator loss is 0.6288979053497314.\n",
            "Epoch 792 of 1000 took 6.765s\n",
            "The Discriminator loss is 2.2451322078704834 and the Generator loss is 1.1535323858261108.\n",
            "Epoch 793 of 1000 took 6.808s\n",
            "The Discriminator loss is 1.849726676940918 and the Generator loss is 1.6580270528793335.\n",
            "Epoch 794 of 1000 took 6.831s\n",
            "The Discriminator loss is 1.500309705734253 and the Generator loss is 1.2860002517700195.\n",
            "Epoch 795 of 1000 took 7.171s\n",
            "The Discriminator loss is 2.479062557220459 and the Generator loss is 0.807493269443512.\n",
            "Epoch 796 of 1000 took 7.091s\n",
            "The Discriminator loss is 3.344984531402588 and the Generator loss is 2.8215088844299316.\n",
            "Epoch 797 of 1000 took 7.193s\n",
            "The Discriminator loss is 2.9614603519439697 and the Generator loss is 0.6230131387710571.\n",
            "Epoch 798 of 1000 took 6.839s\n",
            "The Discriminator loss is 1.0508363246917725 and the Generator loss is 2.521885871887207.\n",
            "Epoch 799 of 1000 took 6.817s\n",
            "The Discriminator loss is 2.4344048500061035 and the Generator loss is 1.7464622259140015.\n",
            "Epoch 800 of 1000 took 6.786s\n",
            "The Discriminator loss is 1.5572426319122314 and the Generator loss is 1.0301964282989502.\n",
            "Epoch 801 of 1000 took 6.702s\n",
            "The Discriminator loss is 2.1204726696014404 and the Generator loss is 4.465491771697998.\n",
            "Now Images are being generated\n",
            "Epoch 802 of 1000 took 6.768s\n",
            "The Discriminator loss is 1.2874544858932495 and the Generator loss is 2.0146303176879883.\n",
            "Epoch 803 of 1000 took 6.801s\n",
            "The Discriminator loss is 3.7788078784942627 and the Generator loss is 1.3348875045776367.\n",
            "Epoch 804 of 1000 took 6.859s\n",
            "The Discriminator loss is 0.9979496002197266 and the Generator loss is 2.315704584121704.\n",
            "Epoch 805 of 1000 took 6.812s\n",
            "The Discriminator loss is 1.6624221801757812 and the Generator loss is 1.39939546585083.\n",
            "Epoch 806 of 1000 took 6.738s\n",
            "The Discriminator loss is 3.528137683868408 and the Generator loss is 0.7639126777648926.\n",
            "Epoch 807 of 1000 took 7.263s\n",
            "The Discriminator loss is 1.9469910860061646 and the Generator loss is 0.8195583820343018.\n",
            "Epoch 808 of 1000 took 7.072s\n",
            "The Discriminator loss is 2.292849540710449 and the Generator loss is 1.9008594751358032.\n",
            "Epoch 809 of 1000 took 6.897s\n",
            "The Discriminator loss is 2.1073920726776123 and the Generator loss is 1.0833635330200195.\n",
            "Epoch 810 of 1000 took 7.035s\n",
            "The Discriminator loss is 0.5958025455474854 and the Generator loss is 2.426176071166992.\n",
            "Epoch 811 of 1000 took 6.899s\n",
            "The Discriminator loss is 1.8210601806640625 and the Generator loss is 0.9759847521781921.\n",
            "Epoch 812 of 1000 took 6.991s\n",
            "The Discriminator loss is 1.718001365661621 and the Generator loss is 3.2633965015411377.\n",
            "Epoch 813 of 1000 took 6.864s\n",
            "The Discriminator loss is 2.482057809829712 and the Generator loss is 1.8817329406738281.\n",
            "Epoch 814 of 1000 took 6.707s\n",
            "The Discriminator loss is 3.1968679428100586 and the Generator loss is 0.6202905774116516.\n",
            "Epoch 815 of 1000 took 6.660s\n",
            "The Discriminator loss is 0.7252296209335327 and the Generator loss is 6.915470123291016.\n",
            "Epoch 816 of 1000 took 6.747s\n",
            "The Discriminator loss is 2.5358409881591797 and the Generator loss is 2.705379009246826.\n",
            "Epoch 817 of 1000 took 6.596s\n",
            "The Discriminator loss is 1.3561198711395264 and the Generator loss is 2.0152390003204346.\n",
            "Epoch 818 of 1000 took 6.512s\n",
            "The Discriminator loss is 2.212392807006836 and the Generator loss is 4.2881317138671875.\n",
            "Epoch 819 of 1000 took 6.596s\n",
            "The Discriminator loss is 1.6428331136703491 and the Generator loss is 1.6838634014129639.\n",
            "Epoch 820 of 1000 took 6.708s\n",
            "The Discriminator loss is 2.2001447677612305 and the Generator loss is 0.7531980872154236.\n",
            "Epoch 821 of 1000 took 6.495s\n",
            "The Discriminator loss is 1.1669658422470093 and the Generator loss is 5.94810676574707.\n",
            "Epoch 822 of 1000 took 6.737s\n",
            "The Discriminator loss is 2.1036880016326904 and the Generator loss is 4.588477611541748.\n",
            "Epoch 823 of 1000 took 6.958s\n",
            "The Discriminator loss is 2.1021246910095215 and the Generator loss is 1.4032273292541504.\n",
            "Epoch 824 of 1000 took 7.023s\n",
            "The Discriminator loss is 1.5075054168701172 and the Generator loss is 1.1369025707244873.\n",
            "Epoch 825 of 1000 took 7.085s\n",
            "The Discriminator loss is 2.1729085445404053 and the Generator loss is 2.2874951362609863.\n",
            "Epoch 826 of 1000 took 7.027s\n",
            "The Discriminator loss is 1.8853249549865723 and the Generator loss is 1.9075431823730469.\n",
            "Epoch 827 of 1000 took 7.124s\n",
            "The Discriminator loss is 2.762190103530884 and the Generator loss is 0.4794641435146332.\n",
            "Epoch 828 of 1000 took 7.168s\n",
            "The Discriminator loss is 2.457491874694824 and the Generator loss is 1.8580302000045776.\n",
            "Epoch 829 of 1000 took 7.116s\n",
            "The Discriminator loss is 1.7958934307098389 and the Generator loss is 1.1591843366622925.\n",
            "Epoch 830 of 1000 took 6.820s\n",
            "The Discriminator loss is 2.778005599975586 and the Generator loss is 0.9677665829658508.\n",
            "Epoch 831 of 1000 took 6.810s\n",
            "The Discriminator loss is 2.530086040496826 and the Generator loss is 1.0345474481582642.\n",
            "Epoch 832 of 1000 took 6.908s\n",
            "The Discriminator loss is 2.104120969772339 and the Generator loss is 1.0471516847610474.\n",
            "Epoch 833 of 1000 took 7.085s\n",
            "The Discriminator loss is 1.7559728622436523 and the Generator loss is 1.1604329347610474.\n",
            "Epoch 834 of 1000 took 7.158s\n",
            "The Discriminator loss is 1.5815937519073486 and the Generator loss is 2.961968421936035.\n",
            "Epoch 835 of 1000 took 7.197s\n",
            "The Discriminator loss is 2.53266978263855 and the Generator loss is 4.05051326751709.\n",
            "Epoch 836 of 1000 took 7.246s\n",
            "The Discriminator loss is 5.21238374710083 and the Generator loss is 0.5035671591758728.\n",
            "Epoch 837 of 1000 took 7.244s\n",
            "The Discriminator loss is 3.145448684692383 and the Generator loss is 0.614391028881073.\n",
            "Epoch 838 of 1000 took 7.166s\n",
            "The Discriminator loss is 2.645975112915039 and the Generator loss is 2.197675943374634.\n",
            "Epoch 839 of 1000 took 7.306s\n",
            "The Discriminator loss is 1.9788875579833984 and the Generator loss is 1.5799751281738281.\n",
            "Epoch 840 of 1000 took 7.250s\n",
            "The Discriminator loss is 1.5765843391418457 and the Generator loss is 2.1075916290283203.\n",
            "Epoch 841 of 1000 took 7.100s\n",
            "The Discriminator loss is 4.292642116546631 and the Generator loss is 0.5476053357124329.\n",
            "Epoch 842 of 1000 took 6.941s\n",
            "The Discriminator loss is 2.360534906387329 and the Generator loss is 1.3861253261566162.\n",
            "Epoch 843 of 1000 took 7.010s\n",
            "The Discriminator loss is 0.42310553789138794 and the Generator loss is 6.816469669342041.\n",
            "Epoch 844 of 1000 took 6.969s\n",
            "The Discriminator loss is 3.3234059810638428 and the Generator loss is 2.708254098892212.\n",
            "Epoch 845 of 1000 took 6.987s\n",
            "The Discriminator loss is 1.712674856185913 and the Generator loss is 1.2258760929107666.\n",
            "Epoch 846 of 1000 took 7.056s\n",
            "The Discriminator loss is 2.2522687911987305 and the Generator loss is 1.1032989025115967.\n",
            "Epoch 847 of 1000 took 7.019s\n",
            "The Discriminator loss is 0.7491521239280701 and the Generator loss is 2.616948127746582.\n",
            "Epoch 848 of 1000 took 7.081s\n",
            "The Discriminator loss is 3.612091302871704 and the Generator loss is 0.537879228591919.\n",
            "Epoch 849 of 1000 took 7.125s\n",
            "The Discriminator loss is 2.0176479816436768 and the Generator loss is 0.8339490294456482.\n",
            "Epoch 850 of 1000 took 6.867s\n",
            "The Discriminator loss is 0.7515588998794556 and the Generator loss is 3.726950168609619.\n",
            "Epoch 851 of 1000 took 7.275s\n",
            "The Discriminator loss is 1.9015705585479736 and the Generator loss is 1.1388823986053467.\n",
            "Epoch 852 of 1000 took 7.290s\n",
            "The Discriminator loss is 1.4859070777893066 and the Generator loss is 2.744363784790039.\n",
            "Epoch 853 of 1000 took 7.001s\n",
            "The Discriminator loss is 1.5943608283996582 and the Generator loss is 3.0856335163116455.\n",
            "Epoch 854 of 1000 took 7.116s\n",
            "The Discriminator loss is 3.01131010055542 and the Generator loss is 0.7991000413894653.\n",
            "Epoch 855 of 1000 took 7.165s\n",
            "The Discriminator loss is 1.5183560848236084 and the Generator loss is 0.9883887767791748.\n",
            "Epoch 856 of 1000 took 6.813s\n",
            "The Discriminator loss is 4.548773765563965 and the Generator loss is 0.5433856248855591.\n",
            "Epoch 857 of 1000 took 6.731s\n",
            "The Discriminator loss is 1.1279964447021484 and the Generator loss is 1.850675106048584.\n",
            "Epoch 858 of 1000 took 6.705s\n",
            "The Discriminator loss is 2.271259307861328 and the Generator loss is 0.8700840473175049.\n",
            "Epoch 859 of 1000 took 6.800s\n",
            "The Discriminator loss is 2.643700361251831 and the Generator loss is 1.1526196002960205.\n",
            "Epoch 860 of 1000 took 6.773s\n",
            "The Discriminator loss is 1.3021636009216309 and the Generator loss is 2.0015687942504883.\n",
            "Epoch 861 of 1000 took 6.651s\n",
            "The Discriminator loss is 1.9423202276229858 and the Generator loss is 1.422623634338379.\n",
            "Epoch 862 of 1000 took 6.829s\n",
            "The Discriminator loss is 1.8126764297485352 and the Generator loss is 1.032859444618225.\n",
            "Epoch 863 of 1000 took 6.758s\n",
            "The Discriminator loss is 1.688717246055603 and the Generator loss is 1.0350446701049805.\n",
            "Epoch 864 of 1000 took 6.689s\n",
            "The Discriminator loss is 1.3753184080123901 and the Generator loss is 2.5967628955841064.\n",
            "Epoch 865 of 1000 took 6.774s\n",
            "The Discriminator loss is 2.129335641860962 and the Generator loss is 0.5998557209968567.\n",
            "Epoch 866 of 1000 took 6.725s\n",
            "The Discriminator loss is 2.941519260406494 and the Generator loss is 0.753072202205658.\n",
            "Epoch 867 of 1000 took 6.790s\n",
            "The Discriminator loss is 1.7204225063323975 and the Generator loss is 2.190967559814453.\n",
            "Epoch 868 of 1000 took 6.922s\n",
            "The Discriminator loss is 0.26148638129234314 and the Generator loss is 4.873251438140869.\n",
            "Epoch 869 of 1000 took 6.793s\n",
            "The Discriminator loss is 1.3794211149215698 and the Generator loss is 2.6044204235076904.\n",
            "Epoch 870 of 1000 took 6.833s\n",
            "The Discriminator loss is 0.37367528676986694 and the Generator loss is 3.7287890911102295.\n",
            "Epoch 871 of 1000 took 6.882s\n",
            "The Discriminator loss is 1.6300528049468994 and the Generator loss is 4.032357215881348.\n",
            "Epoch 872 of 1000 took 6.867s\n",
            "The Discriminator loss is 3.047696590423584 and the Generator loss is 0.9773709774017334.\n",
            "Epoch 873 of 1000 took 6.878s\n",
            "The Discriminator loss is 1.3616575002670288 and the Generator loss is 3.9852378368377686.\n",
            "Epoch 874 of 1000 took 6.775s\n",
            "The Discriminator loss is 2.309129476547241 and the Generator loss is 2.4336700439453125.\n",
            "Epoch 875 of 1000 took 6.754s\n",
            "The Discriminator loss is 2.519667625427246 and the Generator loss is 0.5137258768081665.\n",
            "Epoch 876 of 1000 took 6.794s\n",
            "The Discriminator loss is 2.3805150985717773 and the Generator loss is 0.6225072741508484.\n",
            "Epoch 877 of 1000 took 6.920s\n",
            "The Discriminator loss is 2.0684237480163574 and the Generator loss is 0.9140191078186035.\n",
            "Epoch 878 of 1000 took 7.128s\n",
            "The Discriminator loss is 1.9200451374053955 and the Generator loss is 0.5671123266220093.\n",
            "Epoch 879 of 1000 took 6.866s\n",
            "The Discriminator loss is 2.3101582527160645 and the Generator loss is 1.3142874240875244.\n",
            "Epoch 880 of 1000 took 6.908s\n",
            "The Discriminator loss is 1.25754714012146 and the Generator loss is 2.238521099090576.\n",
            "Epoch 881 of 1000 took 7.137s\n",
            "The Discriminator loss is 3.4277944564819336 and the Generator loss is 0.37316322326660156.\n",
            "Epoch 882 of 1000 took 7.178s\n",
            "The Discriminator loss is 2.1629464626312256 and the Generator loss is 0.7755300998687744.\n",
            "Epoch 883 of 1000 took 7.336s\n",
            "The Discriminator loss is 1.972635269165039 and the Generator loss is 5.857080459594727.\n",
            "Epoch 884 of 1000 took 7.279s\n",
            "The Discriminator loss is 0.8922725915908813 and the Generator loss is 6.559054374694824.\n",
            "Epoch 885 of 1000 took 7.248s\n",
            "The Discriminator loss is 0.5101438760757446 and the Generator loss is 3.9456627368927.\n",
            "Epoch 886 of 1000 took 6.792s\n",
            "The Discriminator loss is 3.8717544078826904 and the Generator loss is 0.7305819988250732.\n",
            "Epoch 887 of 1000 took 6.971s\n",
            "The Discriminator loss is 2.700066566467285 and the Generator loss is 1.385887622833252.\n",
            "Epoch 888 of 1000 took 7.130s\n",
            "The Discriminator loss is 1.6472928524017334 and the Generator loss is 1.214294672012329.\n",
            "Epoch 889 of 1000 took 7.020s\n",
            "The Discriminator loss is 2.6166272163391113 and the Generator loss is 3.3574981689453125.\n",
            "Epoch 890 of 1000 took 7.027s\n",
            "The Discriminator loss is 2.1229586601257324 and the Generator loss is 1.5250614881515503.\n",
            "Epoch 891 of 1000 took 6.948s\n",
            "The Discriminator loss is 1.5645602941513062 and the Generator loss is 1.5395163297653198.\n",
            "Epoch 892 of 1000 took 7.045s\n",
            "The Discriminator loss is 2.8053596019744873 and the Generator loss is 0.6860836744308472.\n",
            "Epoch 893 of 1000 took 7.097s\n",
            "The Discriminator loss is 1.4634974002838135 and the Generator loss is 2.718052625656128.\n",
            "Epoch 894 of 1000 took 7.025s\n",
            "The Discriminator loss is 2.0798470973968506 and the Generator loss is 1.732822299003601.\n",
            "Epoch 895 of 1000 took 6.940s\n",
            "The Discriminator loss is 0.5995498895645142 and the Generator loss is 5.62570333480835.\n",
            "Epoch 896 of 1000 took 7.221s\n",
            "The Discriminator loss is 2.093855857849121 and the Generator loss is 0.7601724863052368.\n",
            "Epoch 897 of 1000 took 6.895s\n",
            "The Discriminator loss is 5.1148223876953125 and the Generator loss is 0.5262387990951538.\n",
            "Epoch 898 of 1000 took 6.777s\n",
            "The Discriminator loss is 2.99271821975708 and the Generator loss is 0.5135647058486938.\n",
            "Epoch 899 of 1000 took 7.112s\n",
            "The Discriminator loss is 1.485194444656372 and the Generator loss is 1.3282999992370605.\n",
            "Epoch 900 of 1000 took 6.877s\n",
            "The Discriminator loss is 2.43061900138855 and the Generator loss is 0.8280326724052429.\n",
            "Epoch 901 of 1000 took 6.606s\n",
            "The Discriminator loss is 1.9692754745483398 and the Generator loss is 0.9535893797874451.\n",
            "Now Images are being generated\n",
            "Epoch 902 of 1000 took 6.742s\n",
            "The Discriminator loss is 0.7731037139892578 and the Generator loss is 9.161147117614746.\n",
            "Epoch 903 of 1000 took 6.587s\n",
            "The Discriminator loss is 1.1615866422653198 and the Generator loss is 2.1061205863952637.\n",
            "Epoch 904 of 1000 took 6.818s\n",
            "The Discriminator loss is 4.711495399475098 and the Generator loss is 1.7299795150756836.\n",
            "Epoch 905 of 1000 took 6.793s\n",
            "The Discriminator loss is 1.1964950561523438 and the Generator loss is 2.600288152694702.\n",
            "Epoch 906 of 1000 took 6.854s\n",
            "The Discriminator loss is 1.4395394325256348 and the Generator loss is 1.7541197538375854.\n",
            "Epoch 907 of 1000 took 6.911s\n",
            "The Discriminator loss is 1.3383660316467285 and the Generator loss is 1.0992225408554077.\n",
            "Epoch 908 of 1000 took 6.886s\n",
            "The Discriminator loss is 2.204338788986206 and the Generator loss is 1.8279006481170654.\n",
            "Epoch 909 of 1000 took 6.855s\n",
            "The Discriminator loss is 2.06467866897583 and the Generator loss is 1.2857431173324585.\n",
            "Epoch 910 of 1000 took 6.797s\n",
            "The Discriminator loss is 1.9854811429977417 and the Generator loss is 0.8114699721336365.\n",
            "Epoch 911 of 1000 took 6.794s\n",
            "The Discriminator loss is 1.790048360824585 and the Generator loss is 2.163120746612549.\n",
            "Epoch 912 of 1000 took 6.680s\n",
            "The Discriminator loss is 1.0971386432647705 and the Generator loss is 7.884119033813477.\n",
            "Epoch 913 of 1000 took 6.834s\n",
            "The Discriminator loss is 1.6211494207382202 and the Generator loss is 3.106879472732544.\n",
            "Epoch 914 of 1000 took 7.026s\n",
            "The Discriminator loss is 2.8200416564941406 and the Generator loss is 1.7092998027801514.\n",
            "Epoch 915 of 1000 took 7.041s\n",
            "The Discriminator loss is 3.2181825637817383 and the Generator loss is 0.5922251343727112.\n",
            "Epoch 916 of 1000 took 7.130s\n",
            "The Discriminator loss is 3.96097993850708 and the Generator loss is 0.7038411498069763.\n",
            "Epoch 917 of 1000 took 7.150s\n",
            "The Discriminator loss is 2.3384346961975098 and the Generator loss is 1.0431636571884155.\n",
            "Epoch 918 of 1000 took 7.230s\n",
            "The Discriminator loss is 2.141164779663086 and the Generator loss is 1.2901027202606201.\n",
            "Epoch 919 of 1000 took 7.147s\n",
            "The Discriminator loss is 2.470092296600342 and the Generator loss is 0.6826512217521667.\n",
            "Epoch 920 of 1000 took 7.077s\n",
            "The Discriminator loss is 1.050089955329895 and the Generator loss is 4.382445335388184.\n",
            "Epoch 921 of 1000 took 7.145s\n",
            "The Discriminator loss is 4.132216453552246 and the Generator loss is 1.105776071548462.\n",
            "Epoch 922 of 1000 took 7.320s\n",
            "The Discriminator loss is 1.9067821502685547 and the Generator loss is 1.2416136264801025.\n",
            "Epoch 923 of 1000 took 7.372s\n",
            "The Discriminator loss is 2.4391825199127197 and the Generator loss is 0.5280663371086121.\n",
            "Epoch 924 of 1000 took 7.240s\n",
            "The Discriminator loss is 2.149428606033325 and the Generator loss is 1.0748729705810547.\n",
            "Epoch 925 of 1000 took 7.153s\n",
            "The Discriminator loss is 2.3103647232055664 and the Generator loss is 1.5719151496887207.\n",
            "Epoch 926 of 1000 took 7.111s\n",
            "The Discriminator loss is 4.993175983428955 and the Generator loss is 0.49701863527297974.\n",
            "Epoch 927 of 1000 took 6.958s\n",
            "The Discriminator loss is 1.6320992708206177 and the Generator loss is 2.4569222927093506.\n",
            "Epoch 928 of 1000 took 6.790s\n",
            "The Discriminator loss is 1.8129750490188599 and the Generator loss is 1.4127060174942017.\n",
            "Epoch 929 of 1000 took 6.913s\n",
            "The Discriminator loss is 1.4095470905303955 and the Generator loss is 1.0513217449188232.\n",
            "Epoch 930 of 1000 took 6.836s\n",
            "The Discriminator loss is 3.1199049949645996 and the Generator loss is 1.4792457818984985.\n",
            "Epoch 931 of 1000 took 6.889s\n",
            "The Discriminator loss is 0.7112884521484375 and the Generator loss is 4.078708171844482.\n",
            "Epoch 932 of 1000 took 6.907s\n",
            "The Discriminator loss is 2.4342241287231445 and the Generator loss is 0.9725728034973145.\n",
            "Epoch 933 of 1000 took 6.814s\n",
            "The Discriminator loss is 1.9847644567489624 and the Generator loss is 0.9649499654769897.\n",
            "Epoch 934 of 1000 took 6.781s\n",
            "The Discriminator loss is 2.981468677520752 and the Generator loss is 1.314860224723816.\n",
            "Epoch 935 of 1000 took 6.742s\n",
            "The Discriminator loss is 2.954677104949951 and the Generator loss is 0.9147589802742004.\n",
            "Epoch 936 of 1000 took 6.738s\n",
            "The Discriminator loss is 3.195862293243408 and the Generator loss is 1.299198031425476.\n",
            "Epoch 937 of 1000 took 6.683s\n",
            "The Discriminator loss is 3.1652634143829346 and the Generator loss is 0.8930408358573914.\n",
            "Epoch 938 of 1000 took 6.591s\n",
            "The Discriminator loss is 1.568568229675293 and the Generator loss is 1.1686931848526.\n",
            "Epoch 939 of 1000 took 6.582s\n",
            "The Discriminator loss is 1.7544584274291992 and the Generator loss is 2.687882661819458.\n",
            "Epoch 940 of 1000 took 6.841s\n",
            "The Discriminator loss is 3.0635952949523926 and the Generator loss is 0.8560305237770081.\n",
            "Epoch 941 of 1000 took 6.818s\n",
            "The Discriminator loss is 3.778841972351074 and the Generator loss is 1.1092344522476196.\n",
            "Epoch 942 of 1000 took 6.629s\n",
            "The Discriminator loss is 2.5537490844726562 and the Generator loss is 1.355627417564392.\n",
            "Epoch 943 of 1000 took 6.727s\n",
            "The Discriminator loss is 2.069019317626953 and the Generator loss is 1.2975170612335205.\n",
            "Epoch 944 of 1000 took 7.378s\n",
            "The Discriminator loss is 3.2940869331359863 and the Generator loss is 0.7008236646652222.\n",
            "Epoch 945 of 1000 took 7.425s\n",
            "The Discriminator loss is 2.5012083053588867 and the Generator loss is 0.5334259271621704.\n",
            "Epoch 946 of 1000 took 6.960s\n",
            "The Discriminator loss is 3.342557430267334 and the Generator loss is 0.4221310317516327.\n",
            "Epoch 947 of 1000 took 6.721s\n",
            "The Discriminator loss is 2.2966973781585693 and the Generator loss is 0.5378361344337463.\n",
            "Epoch 948 of 1000 took 6.985s\n",
            "The Discriminator loss is 1.997983455657959 and the Generator loss is 3.8037919998168945.\n",
            "Epoch 949 of 1000 took 7.083s\n",
            "The Discriminator loss is 1.6315014362335205 and the Generator loss is 1.1271353960037231.\n",
            "Epoch 950 of 1000 took 6.771s\n",
            "The Discriminator loss is 2.5635201930999756 and the Generator loss is 0.7469322085380554.\n",
            "Epoch 951 of 1000 took 6.634s\n",
            "The Discriminator loss is 2.7888216972351074 and the Generator loss is 0.8645867109298706.\n",
            "Epoch 952 of 1000 took 6.680s\n",
            "The Discriminator loss is 1.574069619178772 and the Generator loss is 7.4849090576171875.\n",
            "Epoch 953 of 1000 took 6.911s\n",
            "The Discriminator loss is 1.4328885078430176 and the Generator loss is 2.7170276641845703.\n",
            "Epoch 954 of 1000 took 6.890s\n",
            "The Discriminator loss is 4.058050632476807 and the Generator loss is 1.2565304040908813.\n",
            "Epoch 955 of 1000 took 6.773s\n",
            "The Discriminator loss is 1.3817152976989746 and the Generator loss is 2.7742109298706055.\n",
            "Epoch 956 of 1000 took 6.613s\n",
            "The Discriminator loss is 4.253608703613281 and the Generator loss is 0.6232492923736572.\n",
            "Epoch 957 of 1000 took 6.697s\n",
            "The Discriminator loss is 3.259472370147705 and the Generator loss is 0.6077287197113037.\n",
            "Epoch 958 of 1000 took 6.801s\n",
            "The Discriminator loss is 3.053015947341919 and the Generator loss is 3.425248622894287.\n",
            "Epoch 959 of 1000 took 6.654s\n",
            "The Discriminator loss is 2.2458558082580566 and the Generator loss is 0.7805153131484985.\n",
            "Epoch 960 of 1000 took 6.640s\n",
            "The Discriminator loss is 1.8047353029251099 and the Generator loss is 0.9381470680236816.\n",
            "Epoch 961 of 1000 took 6.661s\n",
            "The Discriminator loss is 3.564234733581543 and the Generator loss is 1.7548826932907104.\n",
            "Epoch 962 of 1000 took 6.663s\n",
            "The Discriminator loss is 3.407014846801758 and the Generator loss is 0.6230894327163696.\n",
            "Epoch 963 of 1000 took 6.626s\n",
            "The Discriminator loss is 2.0298752784729004 and the Generator loss is 1.0651673078536987.\n",
            "Epoch 964 of 1000 took 6.840s\n",
            "The Discriminator loss is 1.7585399150848389 and the Generator loss is 1.380925178527832.\n",
            "Epoch 965 of 1000 took 6.844s\n",
            "The Discriminator loss is 2.3335211277008057 and the Generator loss is 1.7097495794296265.\n",
            "Epoch 966 of 1000 took 6.774s\n",
            "The Discriminator loss is 2.4885034561157227 and the Generator loss is 0.7180683016777039.\n",
            "Epoch 967 of 1000 took 6.813s\n",
            "The Discriminator loss is 2.068749189376831 and the Generator loss is 0.7091349959373474.\n",
            "Epoch 968 of 1000 took 6.887s\n",
            "The Discriminator loss is 3.6948904991149902 and the Generator loss is 1.986080527305603.\n",
            "Epoch 969 of 1000 took 6.927s\n",
            "The Discriminator loss is 1.9482877254486084 and the Generator loss is 0.8201485276222229.\n",
            "Epoch 970 of 1000 took 6.842s\n",
            "The Discriminator loss is 3.139708995819092 and the Generator loss is 2.653237819671631.\n",
            "Epoch 971 of 1000 took 6.855s\n",
            "The Discriminator loss is 2.2306857109069824 and the Generator loss is 0.7491796016693115.\n",
            "Epoch 972 of 1000 took 6.916s\n",
            "The Discriminator loss is 2.835556983947754 and the Generator loss is 0.850907564163208.\n",
            "Epoch 973 of 1000 took 6.935s\n",
            "The Discriminator loss is 0.911029577255249 and the Generator loss is 5.336474418640137.\n",
            "Epoch 974 of 1000 took 7.005s\n",
            "The Discriminator loss is 2.541147232055664 and the Generator loss is 1.5585846900939941.\n",
            "Epoch 975 of 1000 took 7.138s\n",
            "The Discriminator loss is 2.619664430618286 and the Generator loss is 1.8771998882293701.\n",
            "Epoch 976 of 1000 took 7.079s\n",
            "The Discriminator loss is 2.145893096923828 and the Generator loss is 1.217004656791687.\n",
            "Epoch 977 of 1000 took 6.927s\n",
            "The Discriminator loss is 1.2401742935180664 and the Generator loss is 3.598618268966675.\n",
            "Epoch 978 of 1000 took 6.889s\n",
            "The Discriminator loss is 2.770172595977783 and the Generator loss is 5.001662731170654.\n",
            "Epoch 979 of 1000 took 6.939s\n",
            "The Discriminator loss is 4.595242500305176 and the Generator loss is 1.5478644371032715.\n",
            "Epoch 980 of 1000 took 6.934s\n",
            "The Discriminator loss is 0.9996851682662964 and the Generator loss is 3.059084177017212.\n",
            "Epoch 981 of 1000 took 6.972s\n",
            "The Discriminator loss is 1.039076328277588 and the Generator loss is 2.566333532333374.\n",
            "Epoch 982 of 1000 took 6.966s\n",
            "The Discriminator loss is 1.1557121276855469 and the Generator loss is 1.8129265308380127.\n",
            "Epoch 983 of 1000 took 7.044s\n",
            "The Discriminator loss is 3.0111007690429688 and the Generator loss is 0.8729028105735779.\n",
            "Epoch 984 of 1000 took 7.175s\n",
            "The Discriminator loss is 1.1259486675262451 and the Generator loss is 4.011205673217773.\n",
            "Epoch 985 of 1000 took 7.525s\n",
            "The Discriminator loss is 2.115448474884033 and the Generator loss is 1.0778826475143433.\n",
            "Epoch 986 of 1000 took 7.160s\n",
            "The Discriminator loss is 1.9981671571731567 and the Generator loss is 1.541253685951233.\n",
            "Epoch 987 of 1000 took 7.023s\n",
            "The Discriminator loss is 2.256896495819092 and the Generator loss is 0.9363632798194885.\n",
            "Epoch 988 of 1000 took 6.948s\n",
            "The Discriminator loss is 1.8990473747253418 and the Generator loss is 0.8791327476501465.\n",
            "Epoch 989 of 1000 took 7.195s\n",
            "The Discriminator loss is 2.164154052734375 and the Generator loss is 0.786550760269165.\n",
            "Epoch 990 of 1000 took 7.262s\n",
            "The Discriminator loss is 3.84786319732666 and the Generator loss is 1.024623990058899.\n",
            "Epoch 991 of 1000 took 6.920s\n",
            "The Discriminator loss is 1.8588703870773315 and the Generator loss is 3.481715679168701.\n",
            "Epoch 992 of 1000 took 6.972s\n",
            "The Discriminator loss is 2.1867146492004395 and the Generator loss is 1.451056718826294.\n",
            "Epoch 993 of 1000 took 6.995s\n",
            "The Discriminator loss is 2.5889315605163574 and the Generator loss is 0.9784505367279053.\n",
            "Epoch 994 of 1000 took 6.888s\n",
            "The Discriminator loss is 1.6245462894439697 and the Generator loss is 2.691624641418457.\n",
            "Epoch 995 of 1000 took 6.884s\n",
            "The Discriminator loss is 3.918564796447754 and the Generator loss is 0.742785632610321.\n",
            "Epoch 996 of 1000 took 6.837s\n",
            "The Discriminator loss is 2.1592016220092773 and the Generator loss is 0.8237558007240295.\n",
            "Epoch 997 of 1000 took 6.942s\n",
            "The Discriminator loss is 2.2493598461151123 and the Generator loss is 1.1089982986450195.\n",
            "Epoch 998 of 1000 took 7.134s\n",
            "The Discriminator loss is 1.8354105949401855 and the Generator loss is 1.3257304430007935.\n",
            "Epoch 999 of 1000 took 7.083s\n",
            "The Discriminator loss is 1.9146201610565186 and the Generator loss is 2.094052791595459.\n",
            "Epoch 1000 of 1000 took 7.039s\n",
            "The Discriminator loss is 2.1194052696228027 and the Generator loss is 1.271458387374878.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyoYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dztjLGBVFk9e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9dd9e3b5-ba17-40d0-e710-26e4cca5c74c"
      },
      "source": [
        "ls imaginator/"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample400_0.71.png  Sample600_0.28.png  Sample800_0.3.png\n",
            "Sample500_1.33.png  Sample700_1.59.png  Sample900_1.46.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjHDpIXqYgnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94dc030b-4012-44ef-ab63-9d604c074905"
      },
      "source": [
        "from IPython.display import display, Image\n",
        "display(Image(filename='imaginator/Sample400_0.71.png'))\n",
        "display(Image(filename='imaginator/Sample500_1.33.png'))\n",
        "display(Image(filename='imaginator/Sample600_0.28.png'))\n",
        "display(Image(filename='imaginator/Sample700_1.59.png'))\n",
        "display(Image(filename='imaginator/Sample800_0.3.png'))\n",
        "display(Image(filename='imaginator/Sample900_1.46.png'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwEkUGADASRQYAMBJ/wO6wcRfw+VjcgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMACAkygwAICTKDAAgJMoMACAk/4PJHhXp+mIH+cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbAAAAEgCAYAAADVKCZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+8oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79MYq54vabXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dc_Ov-OafIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}